---
title: "`tmfast` fits topic models fast"
author: 
    - name: "Daniel J. Hicks"
      orcid: "0000-0001-7945-4416"
      email: "dhicks4@ucmerced.edu"
      url: "https://dhicks.github.io/"
      affiliations:
          - ref: ucm
      attributes:
          corresponding: true
affiliations: 
    - id: ucm
      name: "University of California, Merced"
      department: "Department of Philosophy"
      address: "5200 N Lake Road"
      city: "Merced, CA"
      country: "USA"
      postal-code: "95343"
abstract: "`tmfast` is an R package for fitting topic models using a fast algorithm based on partial PCA and the varimax rotation.  After providing mathematical background to the method, we present two examples, using a simulated corpus and aggregated works of a selection of authors from the long nineteenth century, and compare the quality of the fitted models to a standard topic modeling package."
date: last-modified
date-format: iso
toc: true
number-sections: true
bibliography: "tmfast.yaml"
format:
    arxiv-pdf: 
        pdf-engine: "lualatex"
        linenumbers: false
        keep-tex: true
        knitr:
            opts_chunk:
                echo: true
        code-block-bg: '#FAFAFA'
        link-citations: true
    # pdf:
    #     code-block-bg: '#FAFAFA'
    html: default
---

# Introduction

Topic modeling is a natural language processing (NLP) technique popular among digital humanists, computational social scientists, and data scientists working with textual data (eg, product reviews) [@RobertsStmPackageStructural2019].  Compared to methods such as vector space embeddings or general-use clustering algorithms such as $k$-means, a key advantage of topic modeling is that it simultaneously clusters both text units (terms or phrases) and documents, enabling analysts to provide human-meaningful, domain-specific labels to the clusters (topics).  

However, a major disadvantage of topic modeling is that the models are relatively computationally intensive and slow to fit.  This strongly discourages analysts from fitting and comparing multiple models, which is arguably the best way to determine to what extent results are sensitive to researcher degrees of freedom [@GelmanGardenForkingPaths2013; @SteegenIncreasingTransparencyMultiverse2016].  Instead, typically analysts fit a few models to a given corpus and focus interpretation on a single "best" model, often chosen by informal assessments of "interpretability" of the fitted topics, introducing additional researcher degrees of freedom.  

This paper reports `tmfast`, an R package designed to facilitate a multiple-model approach by using a significantly faster fitting algorithm.  After giving a brief mathematical background in @sec-math, we walk through two examples of `tmfast` in action:  generating and fitting models to a simulated text corpus (@sec-sim), and then fitting models to a collection of books by different authors retrieved from Project Gutenberg (@sec-realbooks).  Note that both of these examples are supervised cases — the true topics are known *a priori* — and we use a method from @MalaterreEarlyDaysContemporary2022 to assess goodness of fit.  In addition, we also fit models using the `stm` package [@RobertsStmPackageStructural2019] — generally regarded as the state of the art in topic modelling in R — and compare the models fitted by the two packages.  `tmfast` is available at `<https://github.com/dhicks/tmfast>`.  


# Mathematical background {#sec-math}

Topic modeling is typically framed using a generative model.  A corpus $C$ is defined by a fixed vocabulary or collection of terms $T$; a collection of $k$ topics $B$, where each topic $\beta \in B$ is a multinomial distribution over $W$; and parameters $\lambda > 0$ and $\alpha = (\alpha_1, \ldots, \alpha_k)$ with each $\alpha_i > 0$.  Then a document $d$ is generated as follows:  

1. Draw the total length $N_d$ of $d$ from a Poisson distribution, $N_ds \sim \textrm{Poisson}(\lambda)$ (other distributions over the whole numbers might be used here, eg, negative binomial)
2. Draw a ($k$-element) topic distribution $\theta_d$ from the Dirichlet distribution defined by $\alpha$, $\theta_d \sim \textrm{Dir}(\alpha)$
3. For each token $t_i$ ($i = 1, \ldots, N$), 
    a. Draw a topic $b_i \sim \textrm{Multinomial}(\theta_d)$
    b. Draw a term from the topic, $t_i \sim b_i$ [@BleiLatentDirichletAllocation2003 996]. 

This generative model is used to define a joint probability distribution that is fit to the data (observed document lengths and token counts) using numerical methods such as variational Bayes.  

@RoheVintageFactorAnalysis2020 take a different approach to topic modeling, viewing it through the lens of principal component analysis (PCA) and the varimax rotation.  
Consider a rectangular dataset $X$ with $n$ observations of $p$ variables ($n \times p$).  In a statistics or data science context, PCA is used for *dimension reduction*, representing these data with $k < p$ dimensions while preserving as much of the original variance as possible.  Contemporary approaches to PCA use the singular value decomposition

$$ X = U \Sigma V^t = U L^t $$

where $U$ is a $n \times n$ orthogonal matrix (the column vectors are orthogonal and length 1), $\Sigma$ is a $n \times p$ diagonal matrix (all non-diagonal entries are 0), and $V$ is a $p \times p$ orthogonal matrix. $L = V \Sigma^t$ is a $p \times n$ matrix called the *loadings*.  When $p < n$ (that is, more observations than variables) then columns $p+1, p+2, \ldots, n$ of the loadings will be zero, and columns $1, 2, \ldots, p$ can be interpreted as a new set of $p$ variables constructed from the observed $p$ variables.  The rows of $U$ are called the *scores*; they represent the values of the observations in the new variables.  

If $X$ is centered (mean of each column/variable is 0) then the SVD is related to the covariance of the original variables in such a way that the new variables are ordered from greatest to least variance, and the original and new variables have the same total variance.  So if we restrict our attention to the first $k$ new variables we will have a smaller representation of the original dataset that captures as much of the original variance as possible.  Formally, let $U_k$ be the $n \times k$ matrix with columns $1, \ldots, k$ of $U$ and $L_k = V_k \Sigma_k^t$ the corresponding $p \times k$ partial loadings matrix.  Then $X \approx U_k \Sigma_k V_k^t$.  

The loadings matrix is generally not easy to interpret, because the new variables are arbitrary linear combinations of the original variables.  Such interpretations are essential in factor analysis, which attempts to identify interpretable latent variables from the data, such as psychological constructs corresponding to (weighted) sets of items in a survey instrument.^[Strictly speaking, PCA and factor analysis are two different analytical tasks.  Factor analysis models are typically fit by optimizing a maximum likelihood model, rather than an algebraic method like SVD.  And the rotation introduced in the next sentence means that the new variables are not orthogonal/uncorrelated and are not ordered from greatest to least variance, which are key desiderata of PCA.  Nonetheless, the approach to topic modeling proposed by @RoheVintageFactorAnalysis2020 combines PCA with varimax.]  Psychometricians proposed to address this problem by finding a $k \times k$ orthogonal^[Orthogonal matrices have the property that $T^t = T^{-1}$.] matrix $T$ 

$$ U_k L_k^t = U_k T^t T L_k^t = U_k T^t (L_k T)^t $$

that (roughly) makes the "rotated" scores and loadings, $U_k T^t$ and $L_k T$, as *sparse* as possible, that is, have as few non-zero entries as possible.  This makes the new variables much more interpretable, as generalizations or abstractions of a small collection of observed variables.  Because orthogonal matrices generalize rotations and the method for finding this $T$ involves maximizing a total variance, this method is called the *varimax rotation*.  

Finally, to semi-formally motivate a connection between PCA and topic modeling, consider $r_{td}$, the occurrence rate of term $t$ in document $d$.  This rate estimates the conditional probability of $t$ given $d$: 

$$ r_{td} \approx \Pr(t | d) = \sum_i \Pr(t | b_i) \Pr(b_i | d) = \sum_i b_i \theta_d, $$

with a slight abuse of notation, where $i \in 1, \ldots, k$ indexes topics.  In other words, topic modeling can be seen as factoring the (more-or-less observed) term-document distribution into two sets of latent distributions, term-topic and topic-document, much like PCA factors a data matrix into scores and loadings in latent variables.  See @RoheVintageFactorAnalysis2020 lemma 5.2 for a formal development of this connection.  

The upshot is that the latent variables constructed using PCA + varimax can be interpreted as topics.  Sparsity means that a given document will have near-zero value for all but a few topics, and a given topic will have near-zero value for all but a few documents.  

The most obvious potential advantage of this approach is speed.  Text data is typically extremely sparse — documents typically contain only a small fraction of the words in the full vocabulary — and efficient algorithms have been developed for partial SVD of sparse matrices  [@BaglamaAugmentedImplicitlyRestarted2005].  

The `tmfast` package implements this PCA + varimax approach to topic modeling in R, with specific support for the widely-used tidyverse idiom.  The `irlba` package [@BaglamaIrlbaFastTruncated2022] is used for efficient SVD (by default; users can specify an alternative SVD method if they prefer).  `tmfast` is available at <https://github.com/dhicks/tmfast>. 


# Example 1: A simulated corpus {#sec-sim}

`tmfast` includes a collection of functions to generate a simulated corpus according to the standard generative model.  In this section, we use these functions to generate a corpus, fit topic models using `tmfast` and `stm` [@RobertsStmPackageStructural2019] — widely used for topic modeling in R — and compare their respective ability to identify the true topics used to generate the corpus.  

We first load the `tidyverse` suite, the `lpSolve` package to match fitted and true topics, the `tictoc` package to calculate wall compute times, and `tmfast` and `stm`.  The `tidytext` package is also loaded for its `stm` tidiers (eg, functions to represent a fitted `stm` model as a dataframe).  

```{r}
#| message: false
#| warning: false
library(tidyverse)          # infrastructure
theme_set(theme_minimal())  # make plots not look bad
library(lpSolve)            # used to match fitted and true topics
library(tictoc)             # timing
library(tmfast)             # fit topic models fast! 
library(stm)                # standard topic model package
library(tidytext)           # tidiers for stm models
```

## Simulation parameters

We create simulated text data following the data-generating process assumed by LDA.  Specifically, each document will be generated from one of several "journals."  Each journal corresponds to a topic, and vice versa, in that documents from journal $j$ will tend to have a much greater probability for topic $j$ than the other topics.  

We first specify the number of topics/journals `k`, and the number of documents to draw from each journal `Mj`, for a total of `M = Mj * k` documents in the corpus.  We also specify the length of the vocabulary (total unique words) as a multiple of the total number of documents `M`.  Document lengths are generated using a negative binomial distribution, using the size-mean parameterization.  Per `?NegBinomial`, the standard deviation of document lengths in this parameterization is $\sqrt{\mu + \frac{\mu^2}{\mathrm{size}}}$. 

```{r}
k = 10                # Num. topics / journals
Mj = 100              # Num. documents per journal
M = Mj*k              # Total corpus size
vocab = M             # Vocabulary length

## Negative binomial distribution of doc lengths
size = 10             # Size and mean
mu = 300
sqrt(mu + mu^2/size)  # Resulting SD of document sizes
```

Topic-document and word-topic distributions are both sampled from Dirichlet distributions.  For topic-docs, we use an asymmetric Dirichlet distribution^[The $k$-component Dirichlet distribution is parameterized by a $k$-component vector $\mathbf{\alpha} = \alpha (n_1, \ldots, n_k)$, where $\alpha$ is a scalar and $\sum_i n_i = 1$. Using this parameterization, the expected value for component $i$ is $n_i$ with variance $\frac{n_i(1-n_i)}{\alpha + 1}$.  So increasing the scaling factor $\alpha$ means samples from the Dirichlet distribution will be more likely to look like $(n_1, \ldots, n_k)$.] where one component will have (in expectation) most of the probability mass (eg, 80%) and the remaining probability mass will be (in expectation) distributed evenly over the remaining components (eg, $0.2/(k-1)$).  For word-topics we use a symmetric Dirichlet distribution (parametized only by the scaling factor).  `tmfast` includes utility functions for constructing and drawing both kinds of Dirichlet distributions.

```{r}
## Dirichlet distributions for topic-docs and word-topics
topic_peak = .8
topic_scale = 10
peak_alpha(k, 1, peak = topic_peak, scale = topic_scale)
peak_alpha(k, 2, peak = topic_peak, scale = topic_scale)

word_beta = 0.1
```

Because the simulations involve drawing samples using a RNG, we set a seed. 
```{r}
set.seed(2022-06-19)
```

## Draw true topic distributions

We generate the true topic-document distributions $p(\theta = t | \mathrm{doc}_m)$, often simply notated $\theta$ or $\gamma$.  In this vignette we use $\theta$ for the true distribution and $\gamma$ for the fitted distribution in the topic model.  Each document's $\theta$ is sampled from a Dirichlet distribution (`rdirichlet()`), with the parameter $\mathbf{\alpha}$ corresponding to the document's journal $j$.  The variable `theta` is a `M` by `k` matrix; `theta_df` is a tidy representation with columns `doc`, `topic`, and `prob`.  The visualization confirms that documents are generally most strongly associated with the corresponding topics, though with some noise:  in the median document, 82% of its topic probability mass is associated with the single dominant topic.  

```{r}
## Journal-specific alpha, with a peak value (80%) and uniform otherwise; 
## For each topic, draw Mj documents
theta = map(1:k, 
            ~rdirichlet(Mj, 
                        peak_alpha(k, .x, 
                                   peak = topic_peak, 
                                   scale = topic_scale))) |> 
    reduce(rbind)

theta_df = theta |>
    as_tibble(rownames = 'doc', 
              .name_repair = tmfast:::make_colnames) |>
    mutate(doc = as.integer(doc)) |>
    pivot_longer(starts_with('V'),
                 names_to = 'topic',
                 values_to = 'theta')
theta_df

ggplot(theta_df, aes(doc, topic, fill = theta)) +
    geom_tile()

theta_df |> 
    group_by(doc) |> 
    summarize(max = max(theta)) |> 
    pull(max) |> 
    summary()
```

## Draw true word distributions

Next we generate the true word-topic distributions $p(\phi = w | \theta = t)$, often designed as either $\phi$ or $\beta$.  We use $\phi$ for the true distribution and $\beta$ for the fitted distribution.  We sample these distributions from a symmetric Dirichlet distribution over the length of the vocabulary with $\alpha = .01$.  Tile and Zipfian (probability vs. rank on a log-log scale) plots confirm these distributions are working correctly. 

```{r}
## phi_j:  Word distribution for topic j
phi = rdirichlet(k, word_beta, k = vocab)

phi_df = phi |>
    t() |> 
    as_tibble(rownames = 'token', 
              .name_repair = tmfast:::make_colnames) |>
    pivot_longer(starts_with('V'),
                 names_to = 'topic',
                 values_to = 'phi')
phi_df

## Word distributions
ggplot(phi_df, aes(topic, token, fill = (phi))) +
    geom_tile() +
    scale_y_discrete(breaks = NULL)

## Zipfian plot
phi_df |>
    group_by(topic) |>
    mutate(rank = rank(desc(phi))) |>
    arrange(topic, rank) |>
    filter(rank < vocab/2) |>
    ggplot(aes(rank, phi, color = topic)) +
    geom_line() +
    scale_x_log10() +
    scale_y_log10()
```

## Document lengths

Again, document lengths are drawn from a negative binomial distribution. 

```{r}
## N_i:  Length of document i
N = rnbinom(M, size = size, mu = mu)
summary(N)
sd(N)
hist(N)
```

## Draw corpus

Finally we draw the corpus, the observed word counts for each document.  This is the most time-consuming step in this script, much slower than actually fitting the topic model.  Experimenting with this simulation, we found that  `log1p()` scaling of the word counts produced better results than other scaling techniques (eg, dividing by the total length of each document, scaling words by their standard deviation) for accounting for radical differences in document length.^[Since $\log(0+1) = 0$, this transformation also preserves sparsity and does not introduce infinite values.]  

```{r, cache = TRUE}
tic()
corpus = draw_corpus(N, theta, phi)
toc()
dtm = mutate(corpus, n = log1p(n))
```

## Fit the topic model

Fitting the topic model is extremely fast.  Note that we can request multiple values of $k$ (numbers of topics) in a single call.  Other topic modelling packages typically fit only a single value of $k$ at a time.  

Under the hood, we cast the document-term matrix to a sparse matrix class if necessary.  Then we extract the maximum number of desired principal components using `irlba::prcomp_irlba()`, centering but not scaling the logged word counts.  (Experiments with this simulation indicated that scaling makes it more difficult to construct probability distributions later.)  Next we use the base R function `stats:varimax()` to construct a preliminary varimax rotation of the principal components.  Because the direction of factors is arbitrary as far as varimax is concerned, but meaningful when we convert things to probability distributions, we check the skew of each factor's loadings in the preliminary fit, and reverse the factors with negative skew (long left tails with relatively large negative values). 

```{r}
tic()
fitted = tmfast(dtm, c(3, 5, k, 2*k))
toc()
```

The object returned by `tmfast()` has a simple structure (pun intended) and the `tmfast` S3 class.  `totalvar` and `sdev` come from the PCA step, giving the total variance across all feature variables and the standard deviation of each extracted principal component. (Note that these PCs do not generally correspond to the varimax-rotated factors/topics.) `n` contains the sizes (number of factors/topics) fitted for the models, and `varimaxes` contains the varimax fit for each value of `n`. The varimax objects each contain three matrices, the rotated `loadings` (word-topics), the rotation matrix `rotmat`, and the rotated `scores` (document-topics).  Note that these are not stored as probability distributions.  

```{r}
str(fitted, max.level = 2L)
str(fitted$varimax[as.character(k)])
```

Because the model contains a `sdev` component, `screeplot()` works out of the box.  Note that the first $k$ PCs have much higher variance than the others, and often the $k$th PC is somewhat lower than the first $k-1$.  This reflects the highly simplified structure of the simulated data.  Real datasets often have a much more gradual decline in the screeplot, likely reflecting the complex hierarchy of topics in actual documents.  
```{r}
screeplot(fitted, npcs = k + 5)
```

It's also straightforward to calculate the share of total variance covered by successive principal components.  Experimenting with this simulation, it's common for $k$ principal components to cover less than half of the total variance.  Again, note that the rotated varimax factors don't correspond to the principal components, but the total covered variance remains the same.  

```{r}
cumsum(fitted$sdev^2) / fitted$totalvar

data.frame(PC = 1:length(fitted$sdev),
           cum_var = cumsum(fitted$sdev^2) / fitted$totalvar) |> 
    ggplot(aes(PC, cum_var)) +
    geom_line() +
    geom_point()
```

## Fitting a conventional topic model (stm)

For comparison, we'll also fit a conventional topic model using the `stm` package.  To address the challenge of picking a number of topics, `stm::stm()` conducts a topic estimation process when passed `K = 0`.  With the simulation parameters and the random seed used here, this process takes almost 12 seconds and produces a model with 33 topics.  We therefore do not run the code below.  

```{r, eval = FALSE}
tic()
corpus |> 
    cast_sparse(doc, word, n) |> 
    stm(K = 0, verbose = FALSE)
toc()
```

Setting `K = k` gives us a fitted topic model in a few seconds, about an order of magnitude slower than `tmfast()`. Profiling experiments indicated that `tmfast()` is about 20x faster than `stm()`. 

```{r, cache = TRUE}
tic()
fitted_stm = corpus |> 
    cast_sparse(doc, word, n) |> 
    stm(K = k, verbose = FALSE)
toc()
```

## Assessing accuracy: Word-topic distributions

Using simulated data with true word-topic and topic-document distributions enables us to check the accuracy of both `tmfast` and `stm` models.  Here we'll develop a method proposed by @MalaterreEarlyDaysContemporary2022, comparing distributions using Hellinger distance.  For discrete probability distributions $p, q$ over the same space $X$, the Hellinger distance is given by

$$ d(p,q) = \frac{1}{\sqrt{2}} \sqrt{\sum_{x \in X} (\sqrt{p(x)} - \sqrt{q(x)})^2} = \frac{1}{\sqrt{2}} \lVert \sqrt p - \sqrt q \rVert_2. $$

The last equation means that the Hellinger distance is the Euclidean ($L^2$-norm) distance between the *square roots* of the distributions.  Some authors working with topic models sometimes compare distributions using the $L^2$-norm of the distributions themselves, without the square root.  But this approach is flawed, since probability distributions can have different lengths in the $L^2$ norm.  (For example, the distribution $(1, 0)$ has $L^2$ length 1, while $(\frac{1}{2}, \frac{1}{2} )$ has $L^2$ length approximately 1.19.)  Cosine similarity, which is also widely used by text analysts, is directly related to the $L^2$-norm and has the same problem. 

Hellinger distance satisfies the equation
$$ 1 - d^2(p, q) = \sum_{x \in X} \sqrt{p(x)q(x)}. $$
When working with topic models, we're interested in pairwise sets of Hellinger distances, either between all pairs of distributions from a single set [for example, the topic distributions for each document, as used in "discursive space" analysis\; @HicksProductivityInterdisciplinaryImpacts2021] or two sets [such comparing fitted vs. true word-topic distributions as below\; or word-topic distributions for two models fitted on the same corpus but different vocabularies, @MalaterreEarlyDaysContemporary2022].  Working with two sets of distributions $P = \{p_i | i \in I\}$ and $Q = \{q_j | j \in J\}$, the right-hand side of the last equation is equivalent to a matrix multiplication.^[For $P$, each row corresponds to the elementwise square root of one distribution $\sqrt p_i$ and each column to one component $x \in X$, i.e., a cell contains the value $\sqrt{p_i(x)}$.  $Q$ is the transpose, with each row corresponding to one component $x \in X$ and each column corresponding to the square root of a distribution $\sqrt q_j$.  The product of these matrices is a $i \times j$ matrix with each cell the desired sum for $p$ and $q$.]  The `tmfast::hellinger()` function provides S3 methods for calculating Hellinger pairwise distances given a single dataframe, single matrix, or two dataframes or matrices.  

First, however, we need to extract the word-topic distributions.  `tmfast` provides a `tidy()` method, following the pattern of the topic model tidiers in the `tidytext` package.  Unlike other topic models, `tmfast` objects can contain multiple models for different values of $k$.  So, in the second argument to `tidy()`, we need to specify which number of topics we want.  The third argument specifies the desired set of distributions, either word-topics (`'beta'`) or topic-documents (`'gamma'`). 

```{r}
## beta: fitted varimax loadings, transformed to probability distributions
beta = tidy(fitted, k, 'beta')
beta
```

Word-topic distributions correspond to the varimax factor loadings.  These loadings can take any real value.  To convert them to probability distributions, within each factor (topic), we trim negative values to 0 and divide each loading by the sum of all loadings.  The Zipfian plot below compares the fitted and true word-topic distributions.  Consistently across experiments with this simulation, fitted distributions started off a little flatter, then dropped sharply after about 100 words.  In other words, the varimax topic model highlights a relatively long list of characteristic words for each topic — the actual distributions have fewer characteristic words — and then ignores the other words.  

```{r}
## Compare Zipfian distributions
bind_rows(mutate(beta, type = 'fitted'), 
          {phi_df |> 
                  rename(beta = phi) |> 
                  mutate(type = 'true')}) |>
    group_by(type, topic) |>
    mutate(rank = rank(desc(beta))) |>
    arrange(type, topic, rank) |>
    filter(rank < vocab/2) |>
    ggplot(aes(rank, beta, 
               color = type, 
               group = interaction(topic, type))) +
    geom_line() +
    scale_y_log10() +
    scale_x_log10()
```
The Zipfian distribution doesn't tell us which fitted topics might correspond to which true topics.  For that, following @MalaterreEarlyDaysContemporary2022, we'll use pairwise Hellinger distances.  There's one complication, however.  The parameters chosen for this simulation typically end up not drawing some of the words from the vocabulary, and they don't end up in the same order as the true word-topic matrix `phi`.  Fortunately words are represented as the integers `1:vocab`, so it's relatively painless to put them back in order and fill in the gaps (setting the probability for the missing words to be 0 across all topics).  In the code block below, we first fix these issues with the words, widen the long dataframe, convert it to a matrix, and then calculate pairwise Hellinger distances with the true word-topic matrix `phi`. 

```{r}
## Hellinger distance of word-topic distributions
beta_mx = beta |>
    ## Fix order of words
    mutate(token = as.integer(token)) |>
    arrange(token) |>
    ## And dropped words
    complete(token = 1:vocab, topic, fill = list(beta = 0)) |>
    build_matrix(token, topic, beta, sparse = FALSE)

hellinger(phi, t(beta_mx)) |> 
    print(digits = 3)
```

In this distance matrix, the rows are the true topics and the columns are the fitted topics.  Low values correspond to greater similarity.  It's clear that the topics don't match up perfectly — the minimum in each row is about 0.17 — but there is a clear minimum.  We treat this as a linear assignment problem, which is solved rapidly using the `lpSolve` package.  The solution — which matches true to fitted topics — can then be used as a rotation with both the loadings and scores (topic-document distributions).  After rotating, the true-fitted pairs are on the diagonal of the Hellinger distance matrix, making it easy to extract and summarize the quality of the fit.  

```{r}
## Use lpSolve to match fitted topics to true topics
dist = hellinger(phi, t(beta_mx))
rotation = lp.assign(dist)$solution
rotation

## Hellinger distance comparison using the lpSolve matching
hellinger(phi, rotation %*% t(beta_mx)) |> 
    print(digits = 3)
hellinger(phi, rotation %*% t(beta_mx)) |>
    diag() |>
    summary()
```

And we do the same thing with the `stm` topic model.  **`stm` is somewhat more accurate than `tmfast`, with a median Hellinger distance of about 0.07 compared to 0.18.  But `stm` is significantly slower.** 

```{r}
beta_stm_mx = tidy(fitted_stm, matrix = 'beta') |> 
    ## Fix order of words
    mutate(term = as.integer(term)) |>
    arrange(term) |>
    ## And dropped words
    complete(term = 1:vocab, topic, 
             fill = list(beta = 0)) |>
    build_matrix(term, topic, beta, sparse = FALSE)

hellinger(phi, t(beta_stm_mx)) |> 
    print(digits = 3)

rotation_stm = hellinger(phi, t(beta_stm_mx)) |> 
    lp.assign() |> 
    magrittr::extract2('solution')

hellinger(phi, rotation_stm %*% t(beta_stm_mx)) |>
    diag() |>
    summary()
```

The tidied word-topic distributions can be used in standard ways for further analysis, such as a [Silge plot](https://juliasilge.com/blog/2018/2018-01-25-sherlock-holmes-stm_files/figure-html/unnamed-chunk-6-1.png) of the highest probability words for each topic.  But because the "words" in this simulation are just integers, and not semantically meaningful, we don't construct such a plot here.  

## Renormalization: Topic-document distributions 

Finally, we compare fitted and true topic-document distributions.  We extract topic-document distributions using the same `tidy()` function, specifying the matrix `gamma` and including the rotation above to align the fitted and true topics.  Tile and parallel coordinates plots can be used to visualize all of the topic-document distributions.  These show that the `tmfast` models successfully recover the overall association of each document's journal with a distinctive topic.  

```{r}
gamma_df = tidy(fitted, k, 'gamma', 
                rotation = rotation) |> 
    mutate(document = as.integer(document),
           journal = (document - 1) %/% Mj + 1)

ggplot(gamma_df, aes(document, topic, fill = gamma)) +
    geom_raster() +
    scale_x_continuous(breaks = NULL)

ggplot(gamma_df, 
       aes(topic, gamma, 
           group = document, 
           color = as.factor(journal))) +
    geom_line(alpha = .25) +
    facet_wrap(vars(journal)) +
    scale_color_discrete(guide = 'none') +
    scale_x_discrete(guide = 'none')
```

However, the fitted topic-document distributions are flatter than the true ones.  Consider the true and fitted distributions for document 1.  Compared to the true distribution, the fitted distribution has a somewhat lower probability for topic `V01` and a somewhat higher probability for the other topics.  

```{r}
ggplot(mapping = aes(topic, group = 1L)) +
    geom_line(mapping = aes(y = theta, color = 'true'), 
              data = filter(theta_df, doc == '1')) +
    geom_line(mapping = aes(y = gamma, color = 'fitted'), 
              data = filter(gamma_df, document == '1'))
```

This flatter distribution corresponds to greater entropy.  In this simulation, the entropy of the fitted distributions are about 1 bit greater than those of the true distributions.  This discrepancy tends to become worse with greater values of $k$.  

```{r}
theta_df |> 
    group_by(doc) |> 
    summarize(H = entropy(theta)) |> 
    pull(H) |> 
    summary()

gamma_df |> 
    group_by(document) |> 
    summarize(H = entropy(gamma)) |> 
    pull(H) |> 
    summary()
```

To mitigate this problem, we add an optional renormalization step when converting document scores to topic-document distributions.  Given a discrete probability distribution $P$ with components $p_i$ and entropy $H$, and a parameter $\beta$, we can define a new distribution $P'$ with components

$$ p'_i = \frac{p_i^\beta}{\sum_i p_i^\beta} = \frac{p_i^\beta}{Z}$$

which has entropy

$$ H' = \frac{1}{Z} \sum_i [p_i^\beta \beta \log p_i] - \log Z.$$

That is, we can choose a parameter $\beta$ that renormalizes $P$ to achieve a target entropy $H'$.  In LDA, the target entropy is the expected entropy for topic-document distributions drawn from the asymmetric Dirichlet prior.  `tmfast` provides convenience functions for calculating this expected entropy; compare this to the mean entropy of the distributions in `theta` above.  **In actual applications, where the Dirichlet prior is an idealization, choosing $\alpha$ to set the target entropy is an important researcher degree of freedom.**  It is equivalent to choosing prior parameters in other topic modeling packages.  

```{r}
expected_entropy(peak_alpha(k, 1, topic_peak, topic_scale))
```

Since solving the equation for $H'$ for $\beta$ requires numerical optimization, it's inefficient to do this every time we call `tidy()`, especially with large corpora.  Instead, `tmfast::target_power()` is used to run this optimization once, and then return the mean value across all documents.  We then use this single value of $\beta$ in all future calls to `tidy()`.  

```{r}
gamma_power = tidy(fitted, k, 'gamma') |> 
    target_power(document, gamma, 
                 expected_entropy(peak_alpha(k, 
                                             1, 
                                             topic_peak, 
                                             topic_scale)))
gamma_power
```

The renormalized topic-document distributions have closer entropy to $\theta$.  The `keep_original` argument lets us compare the original and renormalized distributions. 

```{r}
gamma_df = tidy(fitted, k, 'gamma', 
                rotation = rotation, 
                exponent = gamma_power, 
                keep_original = TRUE) |> 
    mutate(document = as.integer(document),
           journal = (document - 1) %/% Mj + 1)

gamma_df |> 
    group_by(document) |> 
    summarize(across(c(gamma, gamma_rn), entropy)) |> 
    summarize(across(c(gamma, gamma_rn), mean))
```


We can now assess accuracy of the topic-document distributions.  Above we used the `hellinger()` method for two matrices.  The method for two dataframes requires specifying the id, topic, and probability columns.  The tile plot shows that the true and fitted topics are aligned (because we used the rotation when extracting `gamma_df` above), and so again we can get an overall summary from the diagonal.  Without renormalization, in the current simulation the mean Hellinger distance is 0.24 — not too bad, but perhaps larger than one would like.  With larger values of $k$, this accuracy increases significantly.  Renormalization keeps the mean distance around 0.13, comparable to the word-topic distributions.  

```{r}
## w/o renormalization, mean distance is .24
hellinger(theta_df, doc, prob1 = theta,
          topicsdf2 = gamma_df, 
          id2 = document, 
          prob2 = gamma, df = FALSE) |> 
    diag() |> 
    summary()

## w/ renormalization, mean distance drops to .13
doc_compare = hellinger(theta_df, doc, prob1 = theta,
                        topicsdf2 = gamma_df, 
                        id2 = document, 
                        prob2 = gamma_rn, df = TRUE)

doc_compare |> 
    filter(doc == document) |> 
    pull(dist) |> 
    summary()

ggplot(doc_compare, 
       aes(as.integer(doc), 
           as.integer(document), 
           fill = 1 - dist)) +
    geom_raster() +
    scale_x_discrete(breaks = NULL, name = 'true') +
    scale_y_discrete(breaks = NULL, name = 'fitted')
```

STM has a slightly closer fit, with a mean Hellinger distance of 0.08.  

```{r}
fitted_stm_gamma = tidy(fitted_stm, matrix = 'gamma') |> 
    build_matrix(document, topic, gamma, sparse = FALSE)

hellinger(theta, fitted_stm_gamma %*% t(rotation_stm)) |> 
    diag() |> 
    summary()
```


# Example 2: Literature from the long nineteenth century {#sec-realbooks}

Our second example will analyze a literary corpus from the long nineteenth century, attempting to recover the author of each document.  In order to reduce the number of requests sent to Project Gutenberg, we construct a convenience function to identify and retrieve fulltext works given an author's Gutenberg ID, and wrap this in `memoise::memoise()` to create a local cache.  


```{r}
library(tidyverse)            # infrastructure
theme_set(theme_minimal())
library(ggbeeswarm)
library(memoise)
library(tictoc)
library(glue)

library(gutenbergr)           # text retrieval and manipulation
library(tidytext)
library(tmfast)               # topic modeling
library(stm)                  # topic modeling

get_author = function(author_id) {
    gutenberg_works(gutenberg_author_id == author_id, 
                    has_text) |> 
        gutenberg_download(meta_fields = c('author', 'title'), 
                           mirror = 'http://aleph.gutenberg.org')
}
get_author = memoise(get_author, 
                     cache = cache_filesystem('realbooks'))
```

```{r fig_scale, include=FALSE}
height = 4
aspect_ratio = 1.5
scale = 1.5
knitr::opts_chunk$set(fig.height = height * scale,
                      fig.width = height * aspect_ratio * scale,
                      out.height = glue('{height}in'),
                      out.width = glue('{height * aspect_ratio}in'))
```

## Corpus assembly

We first retrieve all works in Project Gutenberg by our target authors: Jane Austen, Charlotte and Emily Brontë, Louisa May Alcott, George Eliot, Charles Dickens, HG Wells, and HP Lovecraft.  For these authors, the `memoise` local cache ends up at about 286 MB.  

```{r}
## Jane Austen is author 68
# gutenberg_authors |> 
#     filter(str_detect(author, 'Austen'))
austen_df = get_author(68)
```

```{r}
## Anne Brontë is 404
# filter(gutenberg_authors, str_detect(author, 'Brontë'))
a_bronte_df = get_author(404)
```

```{r}
## Charlotte Brontë is 408
# filter(gutenberg_authors, str_detect(author, 'Brontë'))
c_bronte_df = get_author(408)
```

```{r}
## Emily Brontë is 405
# filter(gutenberg_authors, str_detect(author, 'Brontë'))
e_bronte_df = get_author(405)
```

```{r}
## Louisa May Alcott is 102
# filter(gutenberg_authors, str_detect(author, 'Alcott'))
alcott_df = get_author(102)
```

```{r}
## George Eliot is 90
# filter(gutenberg_authors, str_detect(author, 'Eliot'))
eliot_df = get_author(90)
```

```{r}
## Mary Wollstonecraft Shelley is 61
# filter(gutenberg_authors, str_detect(author, 'Shelley'))
shelley_df = get_author(61)
```

```{r}
## Charles Dickens is 37
# filter(gutenberg_authors, str_detect(author, 'Dickens'))
dickens_df = get_author(37)
```

```{r}
## HG Wells is 30
# filter(gutenberg_authors, str_detect(author, 'Wells'))
wells_df = get_author(30)
```

```{r}
## HP Lovecraft is 34724
# filter(gutenberg_authors, str_detect(author, 'Lovecraft'))
lovecraft_df = get_author(34724)
```


We combine these results, and use `tidytext::unnest_tokens()` to convert the result into a long-format document-term matrix.  Note that token extraction can take a long moment.  We also construct a dataframe to link titles to authors in the topic model output. 

```{r}
#| cache: true
dataf = bind_rows(austen_df, 
                  a_bronte_df, 
                  c_bronte_df, 
                  e_bronte_df, 
                  alcott_df, 
                  eliot_df,
                  shelley_df,
                  dickens_df, 
                  wells_df, 
                  lovecraft_df) |> 
    unnest_tokens(term, text, token = 'words') |> 
    count(gutenberg_id, author, title, term)

dataf

meta_df = distinct(dataf, author, title)
```

The number of works by each author varies widely, as does the total token count. 

```{r}
distinct(dataf, author, title) |> 
    count(author)

with(dataf, n_distinct(author, title))
```



```{r}
dataf |> 
    group_by(author, title) |> 
    summarize(n = sum(n)) |> 
    summarize(min = min(n), 
              median = median(n), 
              max = max(n), 
              total = sum(n)) |> 
    arrange(desc(total))

dataf |> 
    group_by(author, title) |> 
    summarize(n = sum(n)) |> 
    ggplot(aes(author, n, color = author)) +
    geom_boxplot() +
    geom_beeswarm() +
    scale_color_discrete(guide = 'none') +
    coord_flip()
```





## Vocabulary selection

In line with a common rule of thumb in topic modeling, we aim for a vocabulary of about 10 times as many terms as documents in the corpus.  

```{r}
vocab_size = n_distinct(dataf$author, dataf$title) * 10
vocab_size
```

`tmfast` provides two information-theoretic methods for vocabulary selection.  Both are based on the idea of a two-player guessing game.  I pick one of the documents from the corpus, then one of the terms from the document.  I tell you the term, and you have to guess which document I picked.  More informative terms have greater information gain (calculated as the Kullback-Leibler divergence) relative to a "baseline" distribution based purely on the process used to pick the document.  The difference between the two methods is in the document-picking process.  The `ndH` method assumes the document was picked uniformly at random from the corpus, so that no document is more likely to be picked than any other.  The `ndR` method assumes document probability is proportional to the document length, so that shorter documents are less likely to be picked.  This method implies that terms that are distinctive of shorter documents have high information gain, since they indicate "surprising" short documents.  

On either method, the most informative terms are often typographical or OCR errors, since these only occur in a single document.  To balance this, we multiply the information gain ($\Delta H$ for the uniform process, $\Delta R$ for the length-weighted process) by the log frequency of the term across the entire corpus ($\log n$).  So `ndH` is shorthand for $\log(n) \Delta H$ while `ndR` is shorthand for $\log(n) \Delta R$. 

```{r}
tic()
H_df = ndH(dataf, title, term, n)
R_df = ndR(dataf, title, term, n) |>
    mutate(in_vocab = rank(desc(ndR)) <= vocab_size)
toc()
H_df
R_df
``` 

The resulting term ranking of the two methods tend to be similar, but `ndR` is preferable in the current case because of the additional weight it gives to distinctive terms from shorter documents.  

```{r}
inner_join(H_df, R_df, by = 'term') |> 
    ggplot(aes(ndH, ndR, color = in_vocab)) +
    geom_point(aes(alpha = rank(desc(ndH)) <= vocab_size))

inner_join(H_df, R_df, by = 'term') |> 
    mutate(ndH_rank = rank(desc(ndH)), 
           ndR_rank = rank(desc(ndR))) |> 
    ggplot(aes(ndH_rank, ndR_rank, color = in_vocab)) +
    geom_point(aes(alpha = ndH_rank <= vocab_size)) +
    scale_x_log10() + 
    scale_y_log10()
```

```{r}
vocab = R_df |> 
    filter(in_vocab) |> 
    pull(term)
head(vocab, 50)
```

```{r}
dataf |> 
    filter(term %in% vocab) |> 
    group_by(author, title) |> 
    summarize(n = sum(n)) |> 
    ggplot(aes(author, n, color = author)) +
    geom_boxplot() +
    geom_beeswarm() +
    scale_color_discrete(guide = 'none') +
    coord_flip()
```


## Fit topic models

```{r}
dtm = dataf |> 
    filter(term %in% vocab) |> 
    mutate(n = log1p(n))

n_authors = n_distinct(dataf$author)

tic()
fitted_tmf = tmfast(dtm, n = c(5,
                               n_authors, 
                               n_authors + 5),
                    row = title, column = term, value = n)
toc()

screeplot(fitted_tmf, npcs = n_authors + 5)
```


## Topic exploration

Without renormalization, most of the works are spread across a few topics, and the topics don't clearly correspond to authors.  

```{r}
tidy(fitted_tmf, n_authors, 'gamma') |> 
    left_join(meta_df, by = c('document' = 'title')) |> 
    ggplot(aes(document, gamma, fill = topic)) +
    geom_col() +
    facet_wrap(vars(author), scales = 'free_x') +
    scale_x_discrete(guide = 'none') +
    scale_fill_viridis_d()
```

To renormalize, we need to choose a theoretical Dirichlet distribution. 

```{r}
alpha = peak_alpha(n_authors, 1, peak = .8, scale = 10)
target_entropy = expected_entropy(alpha)
target_entropy

exponent = tidy(fitted_tmf, n_authors, 'gamma') |> 
    target_power(document, gamma, target_entropy)
exponent

tidy(fitted_tmf, n_authors, 'gamma', exponent = exponent) |> 
    left_join(meta_df, by = c('document' = 'title')) |> 
    ggplot(aes(document, gamma, fill = topic)) +
    geom_col() +
    facet_wrap(vars(author), scales = 'free_x') +
    scale_x_discrete(guide = 'none') +
    scale_fill_viridis_d()
```

```{r}
#| fig-width: !expr height * 2.5 * scale
tidy(fitted_tmf, n_authors, 'gamma', exponent = exponent) |> 
    left_join(meta_df, by = c('document' = 'title')) |> 
    ggplot(aes(document, topic, fill = gamma)) +
    geom_raster() +
    facet_grid(cols = vars(str_wrap(author, 
                                    width = 20)), 
               scales = 'free_x', 
               switch = 'x') +
    scale_x_discrete(guide = 'none')
```

After renormalization, there are distinctive topics for Alcott (4) and Wells (1 and 9).  Austen, Anne Brontë, Emily Brontë, and some of Shelley's works appear together in topic 3.  Charlotte Brontë and some of Eliot's and Shelley's works split topic 5.  Eliot and Lovecraft share topic 10.  And Dickens' works are spread across multiple topics, with 2, 6, and 8 appearing to be distinctive to him.  

To aid interpretation, we create a crosswalk dataframe connecting topics to authors. 
```{r}
topic_author = tribble(
    ~ topic, ~ authors,
    'V01', 'Wells', 
    'V02', 'Dickens', 
    'V03', 'Austin, A & E Brontë', 
    'V04', 'Alcott', 
    'V05', 'Dickens', 
    'V06', 'Dickens', 
    'V07', 'C Brontë, Eliot, Shelley', 
    'V08', 'Dickens', 
    'V09', 'Wells', 
    'V10', 'Eliot, Lovecraft'
)
```

To explore these topics further, we turn to the word-topic distribution.  These distributions could be renormalized, as with the topic-doc distributions.  But the exponent for the word-topic distributions is usually quite close to 1, meaning renormalization doesn't change these distributions very much.  

```{r}
target_entropy_term = expected_entropy(.1, k = vocab_size)
target_entropy_term

exponent_term = tidy(fitted_tmf, n_authors, 'beta') |> 
    target_power(topic, beta, target_entropy_term)
exponent_term
```

We therefore skip renormalization and move directly to a Silge plot, showing the top 10 terms for each topic.  `tidytext::reorder_within()` and `tidytext::scale_x_reordered()` are useful for constructing this plot. 


```{r}
#| dev: cairo_pdf
#| fig-height: !expr height * scale * 1.25
beta_df = tidy(fitted_tmf, n_authors, 'beta')

top_terms = beta_df |> 
    group_by(topic) |> 
    arrange(topic, desc(beta)) |> 
    top_n(15, beta) |> 
    left_join(topic_author, by = 'topic')
top_terms

top_terms |> 
    mutate(token = reorder_within(token, 
                                  by = beta, 
                                  within = topic)) |> 
    ggplot(aes(token, beta)) +
    geom_point() +
    geom_segment(aes(xend = token), yend = 0) +
    facet_wrap(vars(topic, authors), scales = 'free_y') +
    coord_flip() +
    scale_x_reordered()
```

Most topics (2, 3, 4, 5, 6, 8, 9) focus on character names, with three of the four Dickens topics corresponding to *The Pickwick Papers* (topic 2), *Oliver Twist* (5), and *David Copperfield* (8).  Wells' topics appear to distinguish non-fiction essays (topic 1) from fiction (9).  Topic 7 groups together Charlotte Brontë, Eliot, and Shelley based on the use of French. Topic 10 has a mix of character names with months of the year; it appears to be a "miscellaneous" topic, often created by topic models to accommodate documents that don't fit elsewhere.  


# Reproducibility

```{r}
sessioninfo::session_info()
```


# References
