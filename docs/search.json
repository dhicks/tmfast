[{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://dhicks.github.io/tmfast/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://dhicks.github.io/tmfast/articles/realbooks.html","id":"corpus-assembly","dir":"Articles","previous_headings":"","what":"Corpus assembly","title":"Fast topic modeling with real books","text":"first retrieve works Project Gutenberg target authors: Jane Austen, Charlotte Emily Brontë, Louisa May Alcott, George Eliot, Charles Dickens, HG Wells. use memoise package helper function get_author() cache results; cache ends 286 MB. combine results, use tidytext::unnest_tokens() convert result long-format document-term matrix. also construct dataframe link titles authors topic model output. number works author varies widely, total token count.","code":"## Jane Austen is author 68 gutenberg_authors |>      filter(str_detect(author, 'Austen')) ## # A tibble: 7 × 7 ##   gutenberg_author_id author         alias birthdate deathdate wikipedia aliases ##                 <int> <chr>          <chr>     <int>     <int> <chr>     <chr>   ## 1                  68 Austen, Jane   NA         1775      1817 https://… NA      ## 2                7603 Austen-Leigh,… NA         1798      1874 NA        NA      ## 3               25392 Austen-Leigh,… NA           NA        NA NA        NA      ## 4               25393 Austen-Leigh,… Aust…      1872      1961 NA        Austen… ## 5               36446 Austen, Sidney NA           NA        NA NA        NA      ## 6               40288 Layard, Auste… NA         1817      1894 NA        Layard… ## 7               47662 Hubback, Mrs.… NA         1818      1877 NA        Hubbac… austen_df = get_author(68) ## Anne Brontë is 404 filter(gutenberg_authors, str_detect(author, 'Brontë')) ## # A tibble: 4 × 7 ##   gutenberg_author_id author         alias birthdate deathdate wikipedia aliases ##                 <int> <chr>          <chr>     <int>     <int> <chr>     <chr>   ## 1                 404 Brontë, Anne   NA         1820      1849 https://… Bronte… ## 2                 405 Brontë, Emily  NA         1818      1848 https://… Bronte… ## 3                 408 Brontë, Charl… NA         1816      1855 https://… Bell, … ## 4                7125 Brontë, Patri… Bron…      1777      1861 NA        Bronte… a_bronte_df = get_author(404) ## Charlotte Brontë is 408 filter(gutenberg_authors, str_detect(author, 'Brontë')) ## # A tibble: 4 × 7 ##   gutenberg_author_id author         alias birthdate deathdate wikipedia aliases ##                 <int> <chr>          <chr>     <int>     <int> <chr>     <chr>   ## 1                 404 Brontë, Anne   NA         1820      1849 https://… Bronte… ## 2                 405 Brontë, Emily  NA         1818      1848 https://… Bronte… ## 3                 408 Brontë, Charl… NA         1816      1855 https://… Bell, … ## 4                7125 Brontë, Patri… Bron…      1777      1861 NA        Bronte… c_bronte_df = get_author(408) ## Emily Brontë is 405 filter(gutenberg_authors, str_detect(author, 'Brontë')) ## # A tibble: 4 × 7 ##   gutenberg_author_id author         alias birthdate deathdate wikipedia aliases ##                 <int> <chr>          <chr>     <int>     <int> <chr>     <chr>   ## 1                 404 Brontë, Anne   NA         1820      1849 https://… Bronte… ## 2                 405 Brontë, Emily  NA         1818      1848 https://… Bronte… ## 3                 408 Brontë, Charl… NA         1816      1855 https://… Bell, … ## 4                7125 Brontë, Patri… Bron…      1777      1861 NA        Bronte… e_bronte_df = get_author(405) ## Louisa May Alcott is 102 filter(gutenberg_authors, str_detect(author, 'Alcott')) ## # A tibble: 4 × 7 ##   gutenberg_author_id author         alias birthdate deathdate wikipedia aliases ##                 <int> <chr>          <chr>     <int>     <int> <chr>     <chr>   ## 1                 102 Alcott, Louis… NA         1832      1888 https://… Barnar… ## 2                2959 Alcott, Willi… Alco…      1798      1859 https://… Alcott… ## 3               38495 Alcott, Amos … NA         1799      1888 https://… Alcott… ## 4               42641 Pratt, Anna B… NA         1831      1893 NA        NA alcott_df = get_author(102) ## George Eliot is 90 filter(gutenberg_authors, str_detect(author, 'Eliot')) ## # A tibble: 12 × 7 ##    gutenberg_author_id author        alias birthdate deathdate wikipedia aliases ##                  <int> <chr>         <chr>     <int>     <int> <chr>     <chr>   ##  1                  90 Eliot, George NA         1819      1880 https://… Evans,… ##  2                 178 Gregory, Eli… NA         1854      1915 NA        NA      ##  3                 599 Eliot, T. S.… Elio…      1888      1965 https://… Eliot,… ##  4                3546 Eliot, Ethel… NA         1890      1972 NA        NA      ##  5                4887 Eliot, Charl… NA         1862      1931 https://… Eliot,… ##  6                6399 Eliot, Charl… NA         1834      1926 http://e… NA      ##  7                8353 Stokes, Roy … NA           NA        NA NA        NA      ##  8               25235 Robinson, El… NA         1884        NA NA        NA      ##  9               33255 Trumbull, An… NA         1857      1949 NA        NA      ## 10               35592 Howard, Henr… NA         1873      1940 NA        Howard… ## 11               39082 Swan, Mirand… NA           NA        NA NA        NA      ## 12               46750 McCormick, E… NA         1849      1891 NA        NA eliot_df = get_author(90) ## Mary Wollstonecraft Shelley is 61 filter(gutenberg_authors, str_detect(author, 'Shelley')) ## # A tibble: 3 × 7 ##   gutenberg_author_id author         alias birthdate deathdate wikipedia aliases ##                 <int> <chr>          <chr>     <int>     <int> <chr>     <chr>   ## 1                  61 Shelley, Mary… NA         1797      1851 https://… Shelle… ## 2                1529 Shelley, Perc… Shel…      1792      1822 https://… Shelle… ## 3                2162 Shelley, Henr… NA           NA        NA NA        Shelle… shelley_df = get_author(61) ## Charles Dickens is 37 filter(gutenberg_authors, str_detect(author, 'Dickens')) ## # A tibble: 4 × 7 ##   gutenberg_author_id author         alias birthdate deathdate wikipedia aliases ##                 <int> <chr>          <chr>     <int>     <int> <chr>     <chr>   ## 1                  37 Dickens, Char… NA         1812      1870 https://… Boz/Di… ## 2                6323 Clark, Sereno… NA           NA        NA NA        NA      ## 3               31615 Dickens, Mamie Dick…      1838      1896 NA        Dicken… ## 4               47706 Dickens, Mary… NA         1862      1948 NA        NA dickens_df = get_author(37) ## HG Wells is 30 filter(gutenberg_authors, str_detect(author, 'Wells')) ## # A tibble: 30 × 7 ##    gutenberg_author_id author        alias birthdate deathdate wikipedia aliases ##                  <int> <chr>         <chr>     <int>     <int> <chr>     <chr>   ##  1                  30 Wells, H. G.… Well…      1866      1946 https://… Wells,… ##  2                 135 Brown, Willi… NA           NA      1884 https://… Brown,… ##  3                1060 Wells, Carol… Houg…      1862      1942 https://… Hought… ##  4                3499 Wells, Phili… NA         1868      1929 NA        Wells,… ##  5                4952 Wells, J. (J… Well…      1855      1929 https://… Wells,… ##  6                5122 Dall, Caroli… NA         1822      1912 https://… Healey… ##  7                5765 Wells-Barnet… NA         1862      1931 https://… Wells,… ##  8                6158 Hastings, We… Hast…      1879      1923 NA        Hastin… ##  9                7102 Wells, Frede… NA         1874      1929 NA        NA      ## 10               32091 Reeder, Char… NA         1884        NA NA        NA      ## # ℹ 20 more rows wells_df = get_author(30) ## HP Lovecraft is 34724 filter(gutenberg_authors, str_detect(author, 'Lovecraft')) ## # A tibble: 1 × 7 ##   gutenberg_author_id author         alias birthdate deathdate wikipedia aliases ##                 <int> <chr>          <chr>     <int>     <int> <chr>     <chr>   ## 1               34724 Lovecraft, H.… NA         1890      1937 https://… Lovecr… lovecraft_df = get_author(34724) dataf = bind_rows(austen_df,            a_bronte_df,            c_bronte_df,            e_bronte_df,            alcott_df,            eliot_df,           shelley_df,           dickens_df,            wells_df,            lovecraft_df) |>      unnest_tokens(term, text, token = 'words') |>      count(gutenberg_id, author, title, term)  dataf ## # A tibble: 1,812,904 × 5 ##    gutenberg_id author                        title            term            n ##           <int> <chr>                         <chr>            <chr>       <int> ##  1           35 Wells, H. G. (Herbert George) The Time Machine _can_           1 ##  2           35 Wells, H. G. (Herbert George) The Time Machine _cancan_        1 ##  3           35 Wells, H. G. (Herbert George) The Time Machine _down_          1 ##  4           35 Wells, H. G. (Herbert George) The Time Machine _four_          1 ##  5           35 Wells, H. G. (Herbert George) The Time Machine _him_           1 ##  6           35 Wells, H. G. (Herbert George) The Time Machine _how_           1 ##  7           35 Wells, H. G. (Herbert George) The Time Machine _i_             1 ##  8           35 Wells, H. G. (Herbert George) The Time Machine _instantan…     1 ##  9           35 Wells, H. G. (Herbert George) The Time Machine _minus_         1 ## 10           35 Wells, H. G. (Herbert George) The Time Machine _nil_           1 ## # ℹ 1,812,894 more rows meta_df = distinct(dataf, author, title) distinct(dataf, author, title) |>      count(author) ## # A tibble: 10 × 2 ##    author                                 n ##    <chr>                              <int> ##  1 Alcott, Louisa May                    45 ##  2 Austen, Jane                          10 ##  3 Brontë, Anne                           2 ##  4 Brontë, Charlotte                      6 ##  5 Brontë, Emily                          1 ##  6 Dickens, Charles                      77 ##  7 Eliot, George                         18 ##  8 Lovecraft, H. P. (Howard Phillips)     7 ##  9 Shelley, Mary Wollstonecraft          17 ## 10 Wells, H. G. (Herbert George)         70 with(dataf, n_distinct(author, title)) ## [1] 253 dataf |>      group_by(author, title) |>      summarize(n = sum(n)) |>      summarize(min = min(n),                median = median(n),                max = max(n),                total = sum(n)) |>      arrange(desc(total)) ## `summarise()` has grouped output by 'author'. You can override using the ## `.groups` argument. ## # A tibble: 10 × 5 ##    author                                min  median    max   total ##    <chr>                               <int>   <dbl>  <int>   <int> ##  1 Dickens, Charles                     1364  31226  360502 6785632 ##  2 Wells, H. G. (Herbert George)        3958  64936. 470557 5224147 ##  3 Alcott, Louisa May                   2660  55483  194549 2977676 ##  4 Eliot, George                        1871 108236. 320413 2247001 ##  5 Austen, Jane                        23192 101879  784790 1652092 ##  6 Shelley, Mary Wollstonecraft        12514  53643  183856 1434844 ##  7 Brontë, Charlotte                    1416 138921  219783  699938 ##  8 Brontë, Anne                        68716 119946. 171177  239893 ##  9 Lovecraft, H. P. (Howard Phillips)   3654  12073   99008  160200 ## 10 Brontë, Emily                      117082 117082  117082  117082 dataf |>      group_by(author, title) |>      summarize(n = sum(n)) |>      ggplot(aes(author, n, color = author)) +     geom_boxplot() +     geom_beeswarm() +     scale_color_discrete(guide = 'none') +     coord_flip() ## `summarise()` has grouped output by 'author'. You can override using the ## `.groups` argument."},{"path":"https://dhicks.github.io/tmfast/articles/realbooks.html","id":"vocabulary-selection","dir":"Articles","previous_headings":"","what":"Vocabulary selection","title":"Fast topic modeling with real books","text":"line common rule thumb topic modeling, aim vocabulary 10 times many terms documents corpus. tmfast provides two information-theoretic methods vocabulary selection. based idea two-player guessing game. pick one documents corpus, one terms document. tell term, guess document picked. informative terms greater information gain (calculated Kullback-Leibler divergence) relative “baseline” distribution based purely process used pick document. difference two methods document-picking process. ndH method assumes document picked uniformly random corpus, document likely picked . ndR method assumes document probability proportional document length, shorter documents less likely picked. method implies terms distinctive shorter documents high information gain, since indicate “surprising” short documents. either method, informative terms often typographical OCR errors, since occur single document. balance , multiply information gain (\\(\\Delta H\\) uniform process, \\(\\Delta R\\) length-weighted process) log frequency term across entire corpus (\\(\\log n\\)). ndH shorthand \\(\\log(n) \\Delta H\\) ndR shorthand \\(\\log(n) \\Delta R\\). resulting term ranking two methods tend similar, ndR preferable current case additional weight gives distinctive terms shorter documents.","code":"vocab_size = n_distinct(dataf$author, dataf$title) * 10 vocab_size ## [1] 2530 tic() H_df = ndH(dataf, title, term, n) R_df = ndR(dataf, title, term, n) |>     mutate(in_vocab = rank(desc(ndR)) <= vocab_size) toc() ## 18.072 sec elapsed H_df ## # A tibble: 116,449 × 5 ##    term           H    dH     n   ndH ##    <chr>      <dbl> <dbl> <int> <dbl> ##  1 kipps     0.125   7.86  1454  82.6 ##  2 dombey    0.386   7.60  1618  81.0 ##  3 boffin    0.108   7.87  1127  79.8 ##  4 pecksniff 0.366   7.62  1320  79.0 ##  5 gwendolen 0.239   7.74  1048  77.7 ##  6 lydgate   0.0235  7.96   867  77.7 ##  7 deronda   0.404   7.58  1155  77.1 ##  8 nicholas  0.977   7.01  1931  76.5 ##  9 tito      0.0835  7.90   811  76.3 ## 10 squeers   0.253   7.73   895  75.8 ## # ℹ 116,439 more rows R_df ## # A tibble: 116,449 × 5 ##    term           n    dR   ndR in_vocab ##    <chr>      <int> <dbl> <dbl> <lgl>    ##  1 kipps       1454  7.48  78.6 TRUE     ##  2 hoopdriver   469  8.50  75.4 TRUE     ##  3 scrooge     1007  7.54  75.3 TRUE     ##  4 lewisham     575  8.03  73.6 TRUE     ##  5 benham       689  7.56  71.3 TRUE     ##  6 melville     243  8.97  71.1 TRUE     ##  7 bealby       458  8.04  71.0 TRUE     ##  8 veronica     659  7.52  70.4 TRUE     ##  9 bert         556  7.71  70.3 TRUE     ## 10 christie    1797  6.43  69.5 TRUE     ## # ℹ 116,439 more rows inner_join(H_df, R_df, by = 'term') |>      ggplot(aes(ndH, ndR, color = in_vocab)) +     geom_point(aes(alpha = rank(desc(ndH)) <= vocab_size)) ## Warning: Using alpha for a discrete variable is not advised. inner_join(H_df, R_df, by = 'term') |>      mutate(ndH_rank = rank(desc(ndH)),             ndR_rank = rank(desc(ndR))) |>      ggplot(aes(ndH_rank, ndR_rank, color = in_vocab)) +     geom_point(aes(alpha = ndH_rank <= vocab_size)) +     scale_x_log10() +      scale_y_log10() ## Warning: Using alpha for a discrete variable is not advised. vocab = R_df |>      filter(in_vocab) |>      pull(term) head(vocab, 50) ##  [1] \"kipps\"      \"hoopdriver\" \"scrooge\"    \"lewisham\"   \"benham\"     ##  [6] \"melville\"   \"bealby\"     \"veronica\"   \"bert\"       \"christie\"   ## [11] \"sylvia\"     \"snitchey\"   \"boldheart\"  \"britling\"   \"castruccio\" ## [16] \"bounderby\"  \"lillian\"    \"maggie\"     \"marjorie\"   \"craggs\"     ## [21] \"n't\"        \"kemp\"       \"bab\"        \"redwood\"    \"harman\"     ## [26] \"cavor\"      \"chatteris\"  \"brumley\"    \"ammi\"       \"heathcliff\" ## [31] \"tackleton\"  \"gladys\"     \"helwyze\"    \"tetterby\"   \"montgomery\" ## [36] \"lodore\"     \"trafford\"   \"treherne\"   \"jill\"       \"ludovico\"   ## [41] \"tito\"       \"lomi\"       \"canaris\"    \"trotty\"     \"villiers\"   ## [46] \"falkner\"    \"doubledick\" \"amanda\"     \"gradgrind\"  \"linton\" dataf |>      filter(term %in% vocab) |>      group_by(author, title) |>      summarize(n = sum(n)) |>      ggplot(aes(author, n, color = author)) +     geom_boxplot() +     geom_beeswarm() +     scale_color_discrete(guide = 'none') +     coord_flip() ## `summarise()` has grouped output by 'author'. You can override using the ## `.groups` argument."},{"path":"https://dhicks.github.io/tmfast/articles/realbooks.html","id":"fit-topic-models","dir":"Articles","previous_headings":"","what":"Fit topic models","title":"Fast topic modeling with real books","text":"","code":"dtm = dataf |>      filter(term %in% vocab) |>      mutate(n = log1p(n))  n_authors = n_distinct(dataf$author)  tic() fitted_tmf = tmfast(dtm, n = c(5,                               n_authors,                                n_authors + 5),                     row = title, column = term, value = n) toc() ## 0.95 sec elapsed screeplot(fitted_tmf, npcs = n_authors + 5)"},{"path":"https://dhicks.github.io/tmfast/articles/realbooks.html","id":"topic-exploration","dir":"Articles","previous_headings":"","what":"Topic exploration","title":"Fast topic modeling with real books","text":"Without renormalization, works spread across topics, topics don’t clearly correspond authors.  renormalize, need choose theoretical Dirichlet distribution.   renormalization, distinctive topics Alcott (4) Wells (1 9). Austen, Anne Brontë, Emily Brontë, Shelley’s works appear together topic 3. Charlotte Brontë Eliot’s Shelley’s works split topic 5. Eliot Lovecraft share topic 10. Dickens’ works spread across multiple topics, 2, 6, 8 appearing distinctive . aid interpretation, create crosswalk dataframe connecting topics authors. explore topics , turn word-topic distribution. distributions renormalized, topic-doc distributions. exponent word-topic distributions usually quite close 1, meaning renormalization doesn’t change distributions much. therefore skip renormalization move directly Silge plot, showing top 10 terms topic. tidytext::reorder_within() tidytext::scale_x_reordered() useful constructing plot.  topics (2, 3, 4, 5, 6, 8, 9) focus character names, three four Dickens topics corresponding Pickwick Papers (topic 2), Oliver Twist (5), David Copperfield (8). Wells’ topics appear distinguish non-fiction essays (topic 1) fiction (9). Topic 7 groups together Charlotte Brontë, Eliot, Shelley based use French. Topic 10 mix character names months year; appears “miscellaneous” topic, often created topic models accommodate documents don’t fit elsewhere.","code":"tidy(fitted_tmf, n_authors, 'gamma') |>      left_join(meta_df, by = c('document' = 'title')) |>      ggplot(aes(document, gamma, fill = topic)) +     geom_col() +     facet_wrap(vars(author), scales = 'free_x') +     scale_x_discrete(guide = 'none') +     scale_fill_viridis_d() alpha = peak_alpha(n_authors, 1, peak = .8, scale = 10) target_entropy = expected_entropy(alpha) target_entropy ## [1] 0.997604 exponent = tidy(fitted_tmf, n_authors, 'gamma') |>      target_power(document, gamma, target_entropy) exponent ## [1] 4.064884 tidy(fitted_tmf, n_authors, 'gamma', exponent = exponent) |>      left_join(meta_df, by = c('document' = 'title')) |>      ggplot(aes(document, gamma, fill = topic)) +     geom_col() +     facet_wrap(vars(author), scales = 'free_x') +     scale_x_discrete(guide = 'none') +     scale_fill_viridis_d() tidy(fitted_tmf, n_authors, 'gamma', exponent = exponent) |>      left_join(meta_df, by = c('document' = 'title')) |>      ggplot(aes(document, topic, fill = gamma)) +     geom_raster() +     facet_grid(cols = vars(author),                 scales = 'free_x',                 switch = 'x') +     scale_x_discrete(guide = 'none') topic_author = tribble(    ~ topic, ~ authors,    'V01', 'Wells',     'V02', 'Dickens',     'V03', 'Austin, A & E Brontë',     'V04', 'Alcott',     'V05', 'Dickens',     'V06', 'Dickens',     'V07', 'C Brontë, Eliot, Shelley',     'V08', 'Dickens',     'V09', 'Wells',     'V10', 'Eliot, Lovecraft' ) target_entropy_term = expected_entropy(.1, k = vocab_size) target_entropy_term ## [1] 8.597192 exponent_term = tidy(fitted_tmf, n_authors, 'beta') |>      target_power(topic, beta, target_entropy_term) exponent_term ## [1] 1.066448 beta_df = tidy(fitted_tmf, n_authors, 'beta')  top_terms = beta_df |>      group_by(topic) |>      arrange(topic, desc(beta)) |>      top_n(15, beta) |>      left_join(topic_author, by = 'topic') top_terms ## # A tibble: 150 × 4 ## # Groups:   topic [10] ##    token     topic    beta authors ##    <chr>     <chr>   <dbl> <chr>   ##  1 empire    V01   0.0162  Wells   ##  2 britain   V01   0.0124  Wells   ##  3 peoples   V01   0.0122  Wells   ##  4 russia    V01   0.0117  Wells   ##  5 king      V01   0.0111  Wells   ##  6 asia      V01   0.0104  Wells   ##  7 socialism V01   0.00995 Wells   ##  8 section   V01   0.00971 Wells   ##  9 egypt     V01   0.00926 Wells   ## 10 ii        V01   0.00892 Wells   ## # ℹ 140 more rows top_terms |>      mutate(token = reorder_within(token,                                    by = beta,                                    within = topic)) |>      ggplot(aes(token, beta)) +     geom_point() +     geom_segment(aes(xend = token), yend = 0) +     facet_wrap(vars(topic, authors), scales = 'free_y') +     coord_flip() +     scale_x_reordered()"},{"path":[]},{"path":"https://dhicks.github.io/tmfast/articles/simulated.html","id":"simulation-parameters","dir":"Articles","previous_headings":"Simulated text data","what":"Simulation parameters","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"create simulated text data following data-generating process assumed LDA. Specifically, document generated one several “journals.” journal corresponds topic, vice versa, documents journal \\(j\\) tend much greater mixture (probably) topic \\(j\\) topics. first specify number topics/journals k, number documents draw journal Mj, total M = Mj * k documents corpus. also specify length vocabulary (total unique words) multiple total number documents M. Document lengths generated using negative binomial distribution, using size-mean parameterization. Per ?NegBinomial, standard deviation document lengths parameterization \\(\\sqrt{\\mu + \\frac{\\mu^2}{\\mathrm{size}}}\\) simulations involve drawing samples using RNG, first set seed.","code":"k = 10                # Num. topics / journals Mj = 100              # Num. documents per journal M = Mj*k              # Total corpus size vocab = M             # Vocabulary length  ## Negative binomial distribution of doc lengths size = 10             # Size and mean mu = 300 sqrt(mu + mu^2/size)  # Resulting SD of document sizes ## [1] 96.43651 ## Dirichlet distributions for topic-docs and word-topics topic_peak = .8 topic_scale = 10  word_beta = 0.1 set.seed(2022-06-19)"},{"path":"https://dhicks.github.io/tmfast/articles/simulated.html","id":"draw-true-topic-distributions","dir":"Articles","previous_headings":"Simulated text data","what":"Draw true topic distributions","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"first generate true topic-document distributions \\(p(\\theta = t | \\mathrm{doc}_m)\\), often simply called \\(\\theta\\) \\(\\gamma\\). vignette use \\(\\theta\\) true distribution \\(\\gamma\\) fitted distribution topic model. document’s \\(\\theta\\) sampled Dirichlet distribution (rdirichlet()), parameter \\(\\mathbf{\\alpha}\\) corresponding document’s journal \\(j\\). variable theta M k matrix; theta_df tidy representation columns doc, topic, prob. visualization confirms documents generally strongly associated corresponding topics, though noise.","code":"## Journal-specific alpha, with a peak value (.8 by default) and uniform otherwise theta = map(1:k,              ~rdirichlet(Mj, peak_alpha(k, .x,                                         peak = topic_peak,                                         scale = topic_scale))) |>      reduce(rbind)  theta_df = theta |>     as_tibble(rownames = 'doc',                .name_repair = tmfast:::make_colnames) |>     mutate(doc = as.integer(doc)) |>     pivot_longer(starts_with('V'),                  names_to = 'topic',                  values_to = 'prob')  ggplot(theta_df, aes(doc, topic, fill = prob)) +     geom_tile()"},{"path":"https://dhicks.github.io/tmfast/articles/simulated.html","id":"draw-true-word-distributions","dir":"Articles","previous_headings":"Simulated text data","what":"Draw true word distributions","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"Next generate true word-topic distributions \\(p(\\phi = w | \\theta = t)\\), often designed either \\(\\phi\\) \\(\\beta\\). use \\(\\phi\\) true distribution \\(\\beta\\) fitted distribution. sample distributions symmetric Dirichlet distribution length vocabulary \\(\\alpha = .01\\). Tile Zipfian (probability vs. rank log-log scale) plots confirm distributions working correctly.","code":"## phi_j:  Word distribution for topic j phi = rdirichlet(k, word_beta, k = vocab)  ## Word distributions phi |>     as_tibble(rownames = 'topic',                .name_repair = tmfast:::make_colnames) |>     pivot_longer(starts_with('V'),                  names_to = 'word',                  values_to = 'prob') |>     ggplot(aes(topic, word, fill = (prob))) +     geom_tile() +     scale_y_discrete(breaks = NULL) ## Zipf's law phi |>     as_tibble(rownames = 'topic',                .name_repair = \\(x)(str_c('word', 1:vocab))) |>     pivot_longer(starts_with('word'),                  names_to = 'word',                  values_to = 'prob') |>     group_by(topic) |>     mutate(rank = rank(desc(prob))) |>     arrange(topic, rank) |>     filter(rank < vocab/2) |>     ggplot(aes(rank, prob, color = topic)) +     geom_line() +     scale_x_log10() +     scale_y_log10()"},{"path":"https://dhicks.github.io/tmfast/articles/simulated.html","id":"document-lengths","dir":"Articles","previous_headings":"Simulated text data","what":"Document lengths","title":"Fitting topic models (and simulating text data) with `tmfast`","text":", document lengths drawn negative binomial distribution.","code":"## N_i:  Length of document i N = rnbinom(M, size = size, mu = mu) summary(N) ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ##    93.0   240.8   300.5   308.6   364.5   774.0 sd(N) ## [1] 95.1555 hist(N)"},{"path":"https://dhicks.github.io/tmfast/articles/simulated.html","id":"draw-corpus","dir":"Articles","previous_headings":"Simulated text data","what":"Draw corpus","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"Finally draw corpus, observed word counts document. time-consuming step script, much slower actually fitting topic model. Experimenting simulation, found log1p() scaling word counts produced better results scaling techniques (eg, dividing total length document, scaling words standard deviation) accounting radical differences document length.","code":"tic() corpus = draw_corpus(N, theta, phi) toc() ## 27.636 sec elapsed dtm = mutate(corpus, n = log1p(n))"},{"path":"https://dhicks.github.io/tmfast/articles/simulated.html","id":"fit-the-topic-model","dir":"Articles","previous_headings":"","what":"Fit the topic model","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"Fitting topic model extremely fast. Note can request multiple numbers topics single call. hood, first cast document-term matrix (already sparse representation) sparse matrix. extract maximum number desired principal components using irlba::prcomp_irlba(), centering scaling logged word counts. (Experiments simulation indicated scaling makes difficult construct probability distributions later.) irlba implements extremely efficient algorithm partial singular value decompositions large sparse matrices. Next use stats:varimax() construct preliminary varimax rotation principal components. direction factors arbitrary far varimax concerned, meaningful convert things probability distributions, check skew factor’s loadings preliminary fit, reverse factors negative skew (long left tails).1 object returned tmfast() simple structure. totalvar sdev come PCA step, giving total variance across feature variables standard deviation extracted principal component. (Note PCs generally correspond varimax-rotated factors/topics.) n contains sizes (number factors/topics) fitted models, varimaxes contains varimax fit value n. varimax objects contain three matrices, rotated loadings (word-topics), rotation matrix rotmat, rotated scores (document-topics). model contains sdev component, screeplot() works box. Note first \\(k\\) PCs much higher variance others, often \\(k\\)th PC somewhat lower first \\(k-1\\). reflects highly simplified structure simulated data. Real datasets often much gradual decline screeplot, likely reflecting complex hierarchy topics actual documents.  ’s also straightforward calculate share total variance covered successive principal components. Experimenting simulation, documents much larger others, \\(k\\) PCs might cover less half total variance. case covers 65%. , note rotated varimax factors don’t correspond principal components; total covered variance remains .","code":"tic() fitted = tmfast(dtm, c(2, 3, k, 2*k)) toc() ## 0.878 sec elapsed str(fitted, max.level = 2L) ## List of 9 ##  $ totalvar: num 138 ##  $ sdev    : num [1:20] 3.26 3.06 3.01 2.97 2.93 ... ##  $ rows    : chr [1:1000] \"1\" \"2\" \"3\" \"4\" ... ##  $ cols    : chr [1:999] \"5\" \"7\" \"8\" \"11\" ... ##  $ center  : Named num [1:999] 0.4016 0.0943 0.4254 0.2396 0.4093 ... ##   ..- attr(*, \"names\")= chr [1:999] \"5\" \"7\" \"8\" \"11\" ... ##  $ scale   : logi FALSE ##  $ rotation: num [1:999, 1:20] -0.00112 0.02725 0.01223 0.00457 -0.00883 ... ##   ..- attr(*, \"dimnames\")=List of 2 ##  $ n       : num [1:4] 2 3 10 20 ##  $ varimax :List of 4 ##   ..$ 2 :List of 3 ##   ..$ 3 :List of 3 ##   ..$ 10:List of 3 ##   ..$ 20:List of 3 ##  - attr(*, \"class\")= chr [1:3] \"tmfast\" \"varimaxes\" \"list\" str(fitted$varimax$`5`) ##  NULL screeplot(fitted) ## Variance coverage? cumsum(fitted$sdev^2) / fitted$totalvar ##  [1] 0.07689789 0.14466433 0.21018176 0.27420309 0.33636478 0.39492291 ##  [7] 0.45196354 0.50563725 0.55705654 0.57380104 0.57606900 0.57828256 ## [13] 0.58048004 0.58264465 0.58479395 0.58691426 0.58902127 0.59107016 ## [19] 0.59311409 0.59514063 data.frame(PC = 1:length(fitted$sdev),            cum_var = cumsum(fitted$sdev^2) / fitted$totalvar) |>      ggplot(aes(PC, cum_var)) +     geom_line() +     geom_point()"},{"path":"https://dhicks.github.io/tmfast/articles/simulated.html","id":"fitting-a-conventional-topic-model-stm","dir":"Articles","previous_headings":"","what":"Fitting a conventional topic model (stm)","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"comparison, ’ll also fit conventional topic model using stm package. address challenge picking number topics, conduct topic estimation process passed K = 0. simulation parameters random seed used , process takes almost 12 seconds produces model 33 topics. therefore automatically run next chunk. Setting K = k gives us fitted topic model seconds, order magnitude slower tmfast().","code":"tic() corpus |>      cast_sparse(doc, word, n) |>      stm(K = 0, verbose = FALSE) toc() tic() fitted_stm = corpus |>      cast_sparse(doc, word, n) |>      stm(K = k, verbose = FALSE) toc() ## 4.61 sec elapsed"},{"path":"https://dhicks.github.io/tmfast/articles/simulated.html","id":"assessing-accuracy","dir":"Articles","previous_headings":"","what":"Assessing accuracy","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"Using simulated data true word-topic topic-document distributions lets us check accuracy varimax-based topic models. ’ll develop method proposed Malaterre Lareau (2022), comparing distributions using Hellinger distance. discrete probability distributions \\(p, q\\) space \\(X\\), Hellinger distance given \\[ d(p,q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\sum_{x \\X} (\\sqrt{p(x)} - \\sqrt{q(x)})^2} = \\frac{1}{\\sqrt{2}} \\lVert \\sqrt p - \\sqrt q \\rVert_2.\\] last equation means Hellinger distance Euclidean (\\(L^2\\)-norm) distance square roots distributions. authors working topic models sometimes compare distributions using \\(L^2\\)-norm distributions , without square root. approach flawed, since probability distributions can different lengths \\(L^2\\) norm. (example, distribution \\(\\left< 1, 0\\right>\\) \\(L^2\\) length 1, \\(\\left< \\frac{1}{2}, \\frac{1}{2} \\right>\\) \\(L^2\\) length approximately 1.19.)2 Hellinger distance satisfies equation \\[ 1 - d^2(p, q) = \\sum_{x \\X} \\sqrt{p(x)q(x)}. \\] working topic models, ’re interested pairwise sets Hellinger distances, either pairs distributions single set (example, topic distributions document, used “discursive space” analysis [cite xref]) two sets (example, comparing fitted vs. true word-topic distributions, section). Working two sets distributions \\(P = \\{p_i | \\\\}\\) \\(Q = \\{q_j | j \\J\\}\\), right-hand side last equation equivalent matrix multiplication.3 hellinger() function provides S3 methods calculating Hellinger pairwise distances given single dataframe, single matrix, two dataframes matrices. First, however, need extract word-topic distributions. tmfast provides tidy() method, following pattern topicmodel tidiers tidytext. Unlike topic models, tmfast objects can contain models multiple different values \\(k\\) (numbers topics). , second argument tidy(), need specify number topics want. third argument specifies desired set distributions, either word-topics ('beta') topic-documents ('gamma'). Word-topic distributions correspond varimax factor loadings. loadings can take real value. convert probability distributions, within factor (topic), trim negative values 0 divide loading sum loadings. Zipfian plot compares fitted true word-topic distributions. Consistently across experiments simulation, fitted distributions started little flatter, dropped sharply 100 words. words, varimax topic model highlights relatively long list characteristic words topic — actual distributions fewer characteristic words — ignores words. Zipfian distribution doesn’t tell us fitted topics might correspond true topics. , following Malaterre Lareau, ’ll use pairwise Hellinger distances. ’s one complication, however. parameters chosen simulation typically end drawing words vocabulary, don’t end order true word-topic matrix phi. Fortunately words represented integers 1:vocab, ’s relatively painless put back order fill gaps (setting probability missing words 0 across topics). pipe , first fix issues words, widen long dataframe, convert matrix, calculate pairwise Hellinger distances true word-topic matrix phi. distance matrix, rows true topics columns fitted topics. Low values include topics closer . ’s clear topics don’t match perfectly — typically minimum row 0.17 — clear minimum. treat linear assignment problem, solved rapidly using lpSolve package. solution — matches true fitted topics — can used rotation loadings scores (topic-document distributions). rotating, true-fitted pairs diagonal Hellinger distance matrix, making easy extract summarize quality fit. thing conventional topic model. performs somewhat better, median Hellinger distance 0.08. , ’s significantly slower. tidied word-topic distributions can used standard ways analysis, Silge plot highest probability words topic. “words” simulation just integers, semantically meaningful, don’t construct plot .","code":"## beta: fitted varimax loadings, transformed to probability distributions beta = tidy(fitted, k, 'beta') ## Compare Zipfian distributions bind_rows({beta |>         mutate(type = 'fitted')},         {phi |>                 t() |>                 as_tibble(rownames = 'token',                            .name_repair = tmfast:::make_colnames) |>                 pivot_longer(starts_with('V'),                              names_to = 'topic',                              values_to = 'beta') |>                 mutate(beta_rn = beta) |>                  mutate(type = 'true')} ) |>     group_by(type, topic) |>     mutate(rank = rank(desc(beta))) |>     arrange(type, topic, rank) |>     filter(rank < vocab/2) |>     ggplot(aes(rank, beta, color = type,                 group = interaction(topic, type))) +     geom_line() +     scale_y_log10() +     scale_x_log10() ## Hellinger distance of word-topic distributions beta_mx = beta |>     ## Fix order of words     mutate(token = as.integer(token)) |>     arrange(token) |>     ## And dropped words     complete(token = 1:vocab, topic, fill = list(beta = 0)) |>     pivot_wider(names_from = 'topic',                 values_from = 'beta', values_fill = 0,                 names_sort = TRUE) |>     # select(-`NA`) |>     ## Coerce to matrix     column_to_rownames('token') |>     as.matrix()  hellinger(phi, t(beta_mx)) ##             V01       V02       V03       V04       V05       V06       V07 ##  [1,] 0.9031738 0.1637336 0.9165058 0.9123347 0.8751537 0.8995845 0.9036184 ##  [2,] 0.9059649 0.8784121 0.9101200 0.8988053 0.1568935 0.9154690 0.8912032 ##  [3,] 0.9109254 0.8784830 0.9078316 0.8766395 0.8950844 0.8955629 0.8863298 ##  [4,] 0.9364565 0.9175881 0.1665250 0.9072348 0.9071975 0.8779525 0.9121768 ##  [5,] 0.8953244 0.9050252 0.9027020 0.8977285 0.8866189 0.9016970 0.1828639 ##  [6,] 0.9229747 0.9117772 0.8779031 0.8931918 0.9100060 0.1807501 0.9014234 ##  [7,] 0.1642187 0.9068759 0.9378145 0.8921973 0.9020294 0.9310713 0.8995111 ##  [8,] 0.9146167 0.8876726 0.9160216 0.8977089 0.9003917 0.9068250 0.8869320 ##  [9,] 0.8956926 0.9145803 0.9014559 0.1682888 0.8991201 0.9032983 0.8980827 ## [10,] 0.9112636 0.9152296 0.9052323 0.8935218 0.9000401 0.8869707 0.8837047 ##             V08       V09       V10 ##  [1,] 0.9223213 0.8852931 0.8929746 ##  [2,] 0.9044976 0.8865806 0.9026687 ##  [3,] 0.9139487 0.1770633 0.9222647 ##  [4,] 0.9077645 0.9109467 0.9142646 ##  [5,] 0.8797781 0.8815338 0.8907767 ##  [6,] 0.8961334 0.8931672 0.9064453 ##  [7,] 0.9032734 0.9106445 0.9160885 ##  [8,] 0.8796820 0.9247945 0.1708753 ##  [9,] 0.8968698 0.8872700 0.9004598 ## [10,] 0.1616073 0.9255980 0.8824513 ## Use lpSolve to match fitted topics to true topics dist = hellinger(phi, t(beta_mx)) soln = lp.assign(dist) soln$solution ##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ##  [1,]    0    1    0    0    0    0    0    0    0     0 ##  [2,]    0    0    0    0    1    0    0    0    0     0 ##  [3,]    0    0    0    0    0    0    0    0    1     0 ##  [4,]    0    0    1    0    0    0    0    0    0     0 ##  [5,]    0    0    0    0    0    0    1    0    0     0 ##  [6,]    0    0    0    0    0    1    0    0    0     0 ##  [7,]    1    0    0    0    0    0    0    0    0     0 ##  [8,]    0    0    0    0    0    0    0    0    0     1 ##  [9,]    0    0    0    1    0    0    0    0    0     0 ## [10,]    0    0    0    0    0    0    0    1    0     0 hellinger(phi, soln$solution %*% t(beta_mx)) ##            [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7] ##  [1,] 0.1637336 0.8751537 0.8852931 0.9165058 0.9036184 0.8995845 0.9031738 ##  [2,] 0.8784121 0.1568935 0.8865806 0.9101200 0.8912032 0.9154690 0.9059649 ##  [3,] 0.8784830 0.8950844 0.1770633 0.9078316 0.8863298 0.8955629 0.9109254 ##  [4,] 0.9175881 0.9071975 0.9109467 0.1665250 0.9121768 0.8779525 0.9364565 ##  [5,] 0.9050252 0.8866189 0.8815338 0.9027020 0.1828639 0.9016970 0.8953244 ##  [6,] 0.9117772 0.9100060 0.8931672 0.8779031 0.9014234 0.1807501 0.9229747 ##  [7,] 0.9068759 0.9020294 0.9106445 0.9378145 0.8995111 0.9310713 0.1642187 ##  [8,] 0.8876726 0.9003917 0.9247945 0.9160216 0.8869320 0.9068250 0.9146167 ##  [9,] 0.9145803 0.8991201 0.8872700 0.9014559 0.8980827 0.9032983 0.8956926 ## [10,] 0.9152296 0.9000401 0.9255980 0.9052323 0.8837047 0.8869707 0.9112636 ##            [,8]      [,9]     [,10] ##  [1,] 0.8929746 0.9123347 0.9223213 ##  [2,] 0.9026687 0.8988053 0.9044976 ##  [3,] 0.9222647 0.8766395 0.9139487 ##  [4,] 0.9142646 0.9072348 0.9077645 ##  [5,] 0.8907767 0.8977285 0.8797781 ##  [6,] 0.9064453 0.8931918 0.8961334 ##  [7,] 0.9160885 0.8921973 0.9032734 ##  [8,] 0.1708753 0.8977089 0.8796820 ##  [9,] 0.9004598 0.1682888 0.8968698 ## [10,] 0.8824513 0.8935218 0.1616073 hellinger(phi, soln$solution %*% t(beta_mx)) |>     diag() |>     summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ##  0.1569  0.1639  0.1674  0.1693  0.1755  0.1829 beta_stm_mx = tidy(fitted_stm, matrix = 'beta') |>      ## Fix order of words     mutate(term = as.integer(term)) |>     arrange(term) |>     ## And dropped words     complete(term = 1:vocab, topic, fill = list(beta = 0)) |>     pivot_wider(names_from = 'topic',                 values_from = 'beta', values_fill = 0,                 names_sort = TRUE) |>     # select(-`NA`) |>     ## Coerce to matrix     column_to_rownames('term') |>     as.matrix()  hellinger(phi, t(beta_stm_mx)) ##                1          2          3          4          5          6 ##  [1,] 0.08551476 0.84250732 0.84248134 0.88213508 0.87444870 0.87157834 ##  [2,] 0.84329922 0.85367721 0.08226114 0.87003933 0.85853111 0.86964720 ##  [3,] 0.84814338 0.08498287 0.84966302 0.85873155 0.83485457 0.87730942 ##  [4,] 0.88728074 0.86458774 0.87596766 0.08716275 0.86137846 0.90341396 ##  [5,] 0.86509433 0.84154766 0.84198078 0.86693020 0.85183981 0.85492194 ##  [6,] 0.89275208 0.85457918 0.87148785 0.83690625 0.84770470 0.89097927 ##  [7,] 0.87604529 0.87475670 0.86935797 0.89497509 0.86136575 0.08427024 ##  [8,] 0.86179446 0.88513627 0.87104990 0.87765133 0.85763362 0.88206046 ##  [9,] 0.88272319 0.84014092 0.86831204 0.85799803 0.09038475 0.86010142 ## [10,] 0.88217135 0.87848592 0.86296463 0.86880249 0.85155013 0.87344130 ##                7          8          9         10 ##  [1,] 0.85051526 0.86539658 0.87856817 0.86805865 ##  [2,] 0.86158904 0.87025988 0.85982995 0.85353582 ##  [3,] 0.88224633 0.85827133 0.87289308 0.84502101 ##  [4,] 0.87295764 0.83661054 0.86684110 0.87271693 ##  [5,] 0.84603547 0.86047913 0.83835750 0.07902295 ##  [6,] 0.86282228 0.08880347 0.85411829 0.86252234 ##  [7,] 0.87474253 0.89536072 0.86677860 0.85727623 ##  [8,] 0.09267689 0.87054310 0.84452997 0.85300612 ##  [9,] 0.85178500 0.85313348 0.85250779 0.85785724 ## [10,] 0.84600884 0.85587004 0.09097089 0.84596278 rotation_stm = hellinger(phi, t(beta_stm_mx)) |>      lp.assign() |>      magrittr::extract2('solution')  hellinger(phi, rotation_stm %*% t(beta_stm_mx)) |>     diag() |>     summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ## 0.07902 0.08445 0.08634 0.08661 0.08999 0.09268"},{"path":"https://dhicks.github.io/tmfast/articles/simulated.html","id":"topic-document-distributions","dir":"Articles","previous_headings":"","what":"Topic-document distributions","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"extract topic-document distributions using tidy() function, specifying matrix gamma including rotation align fitted true topics.4 Tile parallel coordinates plots can used visualize topic-document distributions. show varimax topic models successfully recover overall association document’s journal distinctive topic.   However, fitted topic-document distributions flatter true ones. Consider true fitted distributions document 1. Compared true distribution, fitted distribution somewhat lower probability topic V01 somewhat higher probability topics.  flatter distribution corresponds greater entropy. simulation, entropy fitted distributions 1 bit greater true distributions. discrepancy tends become worse greater values \\(k\\). mitigate problem, add optional renormalization step converting document scores topic-document distributions. Given discrete probability distribution \\(P\\) components \\(p_i\\) entropy \\(H\\), parameter \\(\\beta\\), can define new distribution \\(P'\\) components \\[ p'_i = \\frac{p_i^\\beta}{\\sum_i p_i^\\beta} = \\frac{p_i^\\beta}{Z}\\] entropy \\[ H' = \\frac{1}{Z} \\sum_i [p_i^\\beta \\beta \\log p_i] - \\log Z.\\] , can choose parameter \\(\\beta\\) renormalizes \\(P\\) achieve target entropy \\(H'\\). LDA, target entropy expected entropy topic-document distributions drawn Dirichlet prior. tmfast provides convenience functions calculating expected entropy; compare mean entropy distributions theta . actual applications, Dirichlet prior idealization, choosing \\(\\alpha\\) set target entropy important researcher degree freedom. equivalent choosing prior parameters topic modeling packages. Since solving equation \\(H'\\) \\(\\beta\\) requires numerical optimization, ’s inefficient every time call tidy(), especially large corpora. Instead, tmfast::target_power() used run optimization , return mean value across documents. use single value \\(\\beta\\) future calls tidy(). renormalized topic-document distributions closer entropy theta. keep_original argument lets us compare original renormalized distributions. can now assess accuracy topic-document distributions. used hellinger() method two matrices. method two dataframes requires specifying id, topic, probability columns. tile plot shows true fitted topics aligned (used rotation extracting gamma_df ), can get overall summary diagonal. Without renormalization, current simulation mean Hellinger distance 0.24 — bad, perhaps larger one like. larger values \\(k\\), accuracy increases significantly. Renormalization keeps mean distance around 0.13, slightly better word-topic distributions.  STM slightly closer fit, mean Hellinger distance 0.08.","code":"gamma_df = tidy(fitted, k, 'gamma',                  rotation = soln$solution) ## Warning in tidy.tmfast(fitted, k, \"gamma\", rotation = soln$solution): Rotating ## scores gamma_df |>     mutate(document = as.integer(document)) |>     ggplot(aes(document, topic, fill = gamma)) +     geom_raster() +     scale_x_continuous(breaks = NULL) gamma_df |>     mutate(document = as.integer(document),            journal = (document - 1) %/% Mj + 1) |>     ggplot(aes(topic, gamma,                 group = document, color = as.factor(journal))) +     geom_line(alpha = .25) +     facet_wrap(vars(journal), scales = 'free_x') +     scale_color_discrete(guide = 'none') ggplot(mapping = aes(topic, group = 1L)) +     geom_line(mapping = aes(y = theta, color = 'true'),                data = tibble(theta = theta[1,],                              topic = tmfast:::make_colnames(1:k))) +     geom_line(mapping = aes(y = gamma, color = 'fitted'),                data = filter(gamma_df, document == '1')) theta |>      apply(1, entropy) |>      summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ##  0.1006  0.6614  0.9715  1.0100  1.3311  2.5821 tidy(fitted, k, 'gamma') |>      group_by(document) |>      summarize(H = entropy(gamma)) |>      pull(H) |>      summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ##  0.8486  1.7040  1.9319  1.9129  2.1592  2.7673 peak_alpha(k, 1, topic_peak, topic_scale) ##  [1] 8.0000000 0.2222222 0.2222222 0.2222222 0.2222222 0.2222222 0.2222222 ##  [8] 0.2222222 0.2222222 0.2222222 expected_entropy(peak_alpha(k, 1, topic_peak, topic_scale)) ## [1] 0.997604 gamma_power = tidy(fitted, k, 'gamma') |>      target_power(document, gamma,                   expected_entropy(peak_alpha(k,                                               1,                                               topic_peak,                                               topic_scale))) gamma_power ## [1] 1.539377 gamma_df = tidy(fitted, k, 'gamma',                  rotation = soln$solution,                  exponent = gamma_power,                  keep_original = TRUE) ## Warning in tidy.tmfast(fitted, k, \"gamma\", rotation = soln$solution, exponent = ## gamma_power, : Rotating scores gamma_df |>      group_by(document) |>      summarize(across(c(gamma, gamma_rn), entropy)) |>      summarize(across(c(gamma, gamma_rn), mean)) ## # A tibble: 1 × 2 ##   gamma gamma_rn ##   <dbl>    <dbl> ## 1  1.91     1.03 ## w/o renormalization, mean distance is .24 hellinger(theta_df, doc,                         topicsdf2 = gamma_df, id2 = document,                          prob2 = gamma, df = FALSE) |>      diag() |>      summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ## 0.08499 0.20131 0.23733 0.23770 0.27244 0.37585 ## w/ renormalization, mean distance drops to .13 doc_compare = hellinger(theta_df, doc,           topicsdf2 = gamma_df, id2 = document,            prob2 = gamma_rn, df = TRUE)  doc_compare |>      filter(doc == document) |>      pull(dist) |>      summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ## 0.04808 0.10195 0.12378 0.12518 0.14564 0.24868 ggplot(doc_compare, aes(as.integer(doc),                          as.integer(document),                          fill = 1 - dist)) +     geom_raster() +     scale_x_discrete(breaks = NULL, name = 'true') +     scale_y_discrete(breaks = NULL, name = 'fitted') fitted_stm_gamma = tidy(fitted_stm, matrix = 'gamma') |>      pivot_wider(names_from = 'topic',                  values_from = 'gamma') |>      column_to_rownames('document') |>      as.matrix()  hellinger(theta, fitted_stm_gamma %*% t(rotation_stm)) |>      diag() |>      summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ## 0.03216 0.07148 0.08638 0.08823 0.10260 0.19884"},{"path":"https://dhicks.github.io/tmfast/articles/simulated.html","id":"discursive-space-visualization-using-t-sne-and-umap","dir":"Articles","previous_headings":"","what":"Discursive space visualization using t-SNE and UMAP","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"Hicks (2021) proposed using topic models Hellinger distance analyze relative position documents “discursive space.” paper used t-SNE dimension-reduction algorithm produce 2D visualization Hellinger distances. UMAP algorithm also popular, believed better t-SNE preserving global structure. tmfast provides functions tsne() umap() efficiently produce visualizations. Note t-SNE algorithm involves random number draws (believe set initial positions points), repeatedly running next chunk produce superficially different visualizations.","code":"tsne(fitted, k) |>     mutate(journal = (as.integer(document)-1) %/% Mj + 1) |>     ggplot(aes(x, y, color = as.character(journal))) +     geom_point() +     labs(title = 't-SNE visualization') umap(fitted, k, df = TRUE) |>      mutate(journal = (as.integer(document)-1) %/% Mj + 1) |>     ggplot(aes(x, y, color = as.character(journal))) +     geom_point() +     labs(title = 'UMAP visualization')"},{"path":[]},{"path":"https://dhicks.github.io/tmfast/articles/test 2.html","id":"simulation-parameters","dir":"Articles","previous_headings":"Simulated text data","what":"Simulation parameters","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"create simulated text data following data-generating process assumed LDA. Specifically, document generated one several “journals.” journal corresponds topic, vice versa, documents journal \\(j\\) tend much greater mixture (probably) topic \\(j\\) topics. first specify number topics/journals k, number documents draw journal Mj, total M = Mj * k documents corpus. also specify length vocabulary (total unique words) multiple total number documents M. Document lengths generated using negative binomial distribution, using size-mean parameterization. Per ?NegBinomial, standard deviation document lengths parameterization \\(\\sqrt{\\mu + \\frac{\\mu^2}{\\mathrm{size}}}\\) simulations involve drawing samples using RNG, first set seed.","code":"k = 10                # Num. topics / journals Mj = 100              # Num. documents per journal M = Mj*k              # Total corpus size vocab = M             # Vocabulary length  ## Negative binomial distribution of doc lengths size = 10             # Size and mean mu = 300 sqrt(mu + mu^2/size)  # Resulting SD of document sizes ## [1] 96.43651 ## Dirichlet distributions for topic-docs and word-topics topic_peak = .8 topic_scale = 10  word_beta = 0.1 set.seed(2022-06-19)"},{"path":"https://dhicks.github.io/tmfast/articles/test 2.html","id":"draw-true-topic-distributions","dir":"Articles","previous_headings":"Simulated text data","what":"Draw true topic distributions","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"first generate true topic-document distributions \\(p(\\theta = t | \\mathrm{doc}_m)\\), often simply called \\(\\theta\\) \\(\\gamma\\). vignette use \\(\\theta\\) true distribution \\(\\gamma\\) fitted distribution topic model. document’s \\(\\theta\\) sampled Dirichlet distribution (rdirichlet()), parameter \\(\\mathbf{\\alpha}\\) corresponding document’s journal \\(j\\). variable theta M k matrix; theta_df tidy representation columns doc, topic, prob. visualization confirms documents generally strongly associated corresponding topics, though noise.","code":"## Journal-specific alpha, with a peak value (.8 by default) and uniform otherwise theta = map(1:k,              ~rdirichlet(Mj, peak_alpha(k, .x,                                         peak = topic_peak,                                         scale = topic_scale))) |>      reduce(rbind)  theta_df = theta |>     as_tibble(rownames = 'doc',                .name_repair = tmfast:::make_colnames) |>     mutate(doc = as.integer(doc)) |>     pivot_longer(starts_with('V'),                  names_to = 'topic',                  values_to = 'prob')  ggplot(theta_df, aes(doc, topic, fill = prob)) +     geom_tile()"},{"path":"https://dhicks.github.io/tmfast/articles/test 2.html","id":"draw-true-word-distributions","dir":"Articles","previous_headings":"Simulated text data","what":"Draw true word distributions","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"Next generate true word-topic distributions \\(p(\\phi = w | \\theta = t)\\), often designed either \\(\\phi\\) \\(\\beta\\). use \\(\\phi\\) true distribution \\(\\beta\\) fitted distribution. sample distributions symmetric Dirichlet distribution length vocabulary \\(\\alpha = .01\\). Tile Zipfian (probability vs. rank log-log scale) plots confirm distributions working correctly.","code":"## phi_j:  Word distribution for topic j phi = rdirichlet(k, word_beta, k = vocab)  ## Word distributions phi |>     as_tibble(rownames = 'topic',                .name_repair = tmfast:::make_colnames) |>     pivot_longer(starts_with('V'),                  names_to = 'word',                  values_to = 'prob') |>     ggplot(aes(topic, word, fill = (prob))) +     geom_tile() +     scale_y_discrete(breaks = NULL) ## Zipf's law phi |>     as_tibble(rownames = 'topic',                .name_repair = \\(x)(str_c('word', 1:vocab))) |>     pivot_longer(starts_with('word'),                  names_to = 'word',                  values_to = 'prob') |>     group_by(topic) |>     mutate(rank = rank(desc(prob))) |>     arrange(topic, rank) |>     filter(rank < vocab/2) |>     ggplot(aes(rank, prob, color = topic)) +     geom_line() +     scale_x_log10() +     scale_y_log10()"},{"path":"https://dhicks.github.io/tmfast/articles/test 2.html","id":"document-lengths","dir":"Articles","previous_headings":"Simulated text data","what":"Document lengths","title":"Fitting topic models (and simulating text data) with `tmfast`","text":", document lengths drawn negative binomial distribution.","code":"## N_i:  Length of document i N = rnbinom(M, size = size, mu = mu) summary(N) ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ##    93.0   240.8   300.5   308.6   364.5   774.0 sd(N) ## [1] 95.1555 hist(N)"},{"path":"https://dhicks.github.io/tmfast/articles/test 2.html","id":"draw-corpus","dir":"Articles","previous_headings":"Simulated text data","what":"Draw corpus","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"Finally draw corpus, observed word counts document. time-consuming step script, much slower actually fitting topic model. Experimenting simulation, found log1p() scaling word counts produced better results scaling techniques (eg, dividing total length document, scaling words standard deviation) accounting radical differences document length.","code":"tic() corpus = draw_corpus(N, theta, phi) toc() ## 26.798 sec elapsed dtm = mutate(corpus, n = log1p(n))"},{"path":"https://dhicks.github.io/tmfast/articles/test 2.html","id":"fit-the-topic-model","dir":"Articles","previous_headings":"","what":"Fit the topic model","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"Fitting topic model extremely fast. Note can request multiple numbers topics single call. hood, first cast document-term matrix (already sparse representation) sparse matrix. extract maximum number desired principal components using irlba::prcomp_irlba(), centering scaling logged word counts. (Experiments simulation indicated scaling makes difficult construct probability distributions later.) irlba implements extremely efficient algorithm partial singular value decompositions large sparse matrices. Next use stats:varimax() construct preliminary varimax rotation principal components. direction factors arbitrary far varimax concerned, meaningful convert things probability distributions, check skew factor’s loadings preliminary fit, reverse factors negative skew (long left tails).1 object returned tmfast() simple structure. totalvar sdev come PCA step, giving total variance across feature variables standard deviation extracted principal component. (Note PCs generally correspond varimax-rotated factors/topics.) n contains sizes (number factors/topics) fitted models, varimaxes contains varimax fit value n. varimax objects contain three matrices, rotated loadings (word-topics), rotation matrix rotmat, rotated scores (document-topics). model contains sdev component, screeplot() works box. Note first \\(k\\) PCs much higher variance others, often \\(k\\)th PC somewhat lower first \\(k-1\\). reflects highly simplified structure simulated data. Real datasets often much gradual decline screeplot, likely reflecting complex hierarchy topics actual documents.  ’s also straightforward calculate share total variance covered successive principal components. Experimenting simulation, documents much larger others, \\(k\\) PCs might cover less half total variance. case covers 65%. , note rotated varimax factors don’t correspond principal components; total covered variance remains .","code":"tic() fitted = tmfast(dtm, c(2, 3, k, 2*k)) toc() ## 0.665 sec elapsed str(fitted, max.level = 2L) ## List of 9 ##  $ totalvar: num 138 ##  $ sdev    : num [1:20] 3.26 3.06 3.01 2.97 2.93 ... ##  $ rows    : chr [1:1000] \"1\" \"2\" \"3\" \"4\" ... ##  $ cols    : chr [1:999] \"5\" \"7\" \"8\" \"11\" ... ##  $ center  : Named num [1:999] 0.4016 0.0943 0.4254 0.2396 0.4093 ... ##   ..- attr(*, \"names\")= chr [1:999] \"5\" \"7\" \"8\" \"11\" ... ##  $ scale   : logi FALSE ##  $ rotation: num [1:999, 1:20] -0.00112 0.02725 0.01223 0.00457 -0.00883 ... ##   ..- attr(*, \"dimnames\")=List of 2 ##  $ n       : num [1:4] 2 3 10 20 ##  $ varimax :List of 4 ##   ..$ 2 :List of 3 ##   ..$ 3 :List of 3 ##   ..$ 10:List of 3 ##   ..$ 20:List of 3 ##  - attr(*, \"class\")= chr [1:3] \"tmfast\" \"varimaxes\" \"list\" str(fitted$varimax$`5`) ##  NULL screeplot(fitted) ## Variance coverage? cumsum(fitted$sdev^2) / fitted$totalvar ##  [1] 0.07689789 0.14466433 0.21018176 0.27420309 0.33636478 0.39492291 ##  [7] 0.45196354 0.50563725 0.55705654 0.57380104 0.57606900 0.57828256 ## [13] 0.58048004 0.58264465 0.58479395 0.58691426 0.58902127 0.59107016 ## [19] 0.59311409 0.59514063 data.frame(PC = 1:length(fitted$sdev),            cum_var = cumsum(fitted$sdev^2) / fitted$totalvar) |>      ggplot(aes(PC, cum_var)) +     geom_line() +     geom_point()"},{"path":"https://dhicks.github.io/tmfast/articles/test 2.html","id":"fitting-a-conventional-topic-model-stm","dir":"Articles","previous_headings":"","what":"Fitting a conventional topic model (stm)","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"comparison, ’ll also fit conventional topic model using stm package. address challenge picking number topics, conduct topic estimation process passed K = 0. simulation parameters random seed used , process takes almost 12 seconds produces model 33 topics. therefore automatically run next chunk. Setting K = k gives us fitted topic model seconds, order magnitude slower tmfast().","code":"tic() corpus |>      cast_sparse(doc, word, n) |>      stm(K = 0, verbose = FALSE) toc() tic() fitted_stm = corpus |>      cast_sparse(doc, word, n) |>      stm(K = k, verbose = FALSE) toc() ## 4.716 sec elapsed"},{"path":"https://dhicks.github.io/tmfast/articles/test 2.html","id":"assessing-accuracy","dir":"Articles","previous_headings":"","what":"Assessing accuracy","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"Using simulated data true word-topic topic-document distributions lets us check accuracy varimax-based topic models. ’ll develop method proposed Malaterre Lareau (2022), comparing distributions using Hellinger distance. discrete probability distributions \\(p, q\\) space \\(X\\), Hellinger distance given \\[ d(p,q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\sum_{x \\X} (\\sqrt{p(x)} - \\sqrt{q(x)})^2} = \\frac{1}{\\sqrt{2}} \\lVert \\sqrt p - \\sqrt q \\rVert_2.\\] last equation means Hellinger distance Euclidean (\\(L^2\\)-norm) distance square roots distributions. authors working topic models sometimes compare distributions using \\(L^2\\)-norm distributions , without square root. approach flawed, since probability distributions can different lengths \\(L^2\\) norm. (example, distribution \\(\\left< 1, 0\\right>\\) \\(L^2\\) length 1, \\(\\left< \\frac{1}{2}, \\frac{1}{2} \\right>\\) \\(L^2\\) length approximately 1.19.)2 Hellinger distance satisfies equation \\[ 1 - d^2(p, q) = \\sum_{x \\X} \\sqrt{p(x)q(x)}. \\] working topic models, ’re interested pairwise sets Hellinger distances, either pairs distributions single set (example, topic distributions document, used “discursive space” analysis [cite xref]) two sets (example, comparing fitted vs. true word-topic distributions, section). Working two sets distributions \\(P = \\{p_i | \\\\}\\) \\(Q = \\{q_j | j \\J\\}\\), right-hand side last equation equivalent matrix multiplication.3 hellinger() function provides S3 methods calculating Hellinger pairwise distances given single dataframe, single matrix, two dataframes matrices. First, however, need extract word-topic distributions. tmfast provides tidy() method, following pattern topicmodel tidiers tidytext. Unlike topic models, tmfast objects can contain models multiple different values \\(k\\) (numbers topics). , second argument tidy(), need specify number topics want. third argument specifies desired set distributions, either word-topics ('beta') topic-documents ('gamma'). Word-topic distributions correspond varimax factor loadings. loadings can take real value. convert probability distributions, within factor (topic), trim negative values 0 divide loading sum loadings. Zipfian plot compares fitted true word-topic distributions. Consistently across experiments simulation, fitted distributions started little flatter, dropped sharply 100 words. words, varimax topic model highlights relatively long list characteristic words topic — actual distributions fewer characteristic words — ignores words. Zipfian distribution doesn’t tell us fitted topics might correspond true topics. , following Malaterre Lareau, ’ll use pairwise Hellinger distances. ’s one complication, however. parameters chosen simulation typically end drawing words vocabulary, don’t end order true word-topic matrix phi. Fortunately words represented integers 1:vocab, ’s relatively painless put back order fill gaps (setting probability missing words 0 across topics). pipe , first fix issues words, widen long dataframe, convert matrix, calculate pairwise Hellinger distances true word-topic matrix phi. distance matrix, rows true topics columns fitted topics. Low values include topics closer . ’s clear topics don’t match perfectly — typically minimum row 0.17 — clear minimum. treat linear assignment problem, solved rapidly using lpSolve package. solution — matches true fitted topics — can used rotation loadings scores (topic-document distributions). rotating, true-fitted pairs diagonal Hellinger distance matrix, making easy extract summarize quality fit. thing conventional topic model. performs somewhat better, median Hellinger distance 0.08. , ’s significantly slower. tidied word-topic distributions can used standard ways analysis, Silge plot highest probability words topic. “words” simulation just integers, semantically meaningful, don’t construct plot .","code":"## beta: fitted varimax loadings, transformed to probability distributions beta = tidy(fitted, k, 'beta') ## Compare Zipfian distributions bind_rows({beta |>         mutate(type = 'fitted')},         {phi |>                 t() |>                 as_tibble(rownames = 'token',                            .name_repair = tmfast:::make_colnames) |>                 pivot_longer(starts_with('V'),                              names_to = 'topic',                              values_to = 'beta') |>                 mutate(beta_rn = beta) |>                  mutate(type = 'true')} ) |>     group_by(type, topic) |>     mutate(rank = rank(desc(beta))) |>     arrange(type, topic, rank) |>     filter(rank < vocab/2) |>     ggplot(aes(rank, beta, color = type,                 group = interaction(topic, type))) +     geom_line() +     scale_y_log10() +     scale_x_log10() ## Hellinger distance of word-topic distributions beta_mx = beta |>     ## Fix order of words     mutate(token = as.integer(token)) |>     arrange(token) |>     ## And dropped words     complete(token = 1:vocab, topic, fill = list(beta = 0)) |>     pivot_wider(names_from = 'topic',                 values_from = 'beta', values_fill = 0,                 names_sort = TRUE) |>     # select(-`NA`) |>     ## Coerce to matrix     column_to_rownames('token') |>     as.matrix()  hellinger(phi, t(beta_mx)) ##             V01       V02       V03       V04       V05       V06       V07 ##  [1,] 0.9031738 0.1637336 0.9165058 0.9123347 0.8751537 0.8995845 0.9036184 ##  [2,] 0.9059649 0.8784121 0.9101200 0.8988053 0.1568935 0.9154690 0.8912032 ##  [3,] 0.9109254 0.8784830 0.9078316 0.8766395 0.8950844 0.8955629 0.8863298 ##  [4,] 0.9364565 0.9175881 0.1665250 0.9072348 0.9071975 0.8779525 0.9121768 ##  [5,] 0.8953244 0.9050252 0.9027020 0.8977285 0.8866189 0.9016970 0.1828639 ##  [6,] 0.9229747 0.9117772 0.8779031 0.8931918 0.9100060 0.1807501 0.9014234 ##  [7,] 0.1642187 0.9068759 0.9378145 0.8921973 0.9020294 0.9310713 0.8995111 ##  [8,] 0.9146167 0.8876726 0.9160216 0.8977089 0.9003917 0.9068250 0.8869320 ##  [9,] 0.8956926 0.9145803 0.9014559 0.1682888 0.8991201 0.9032983 0.8980827 ## [10,] 0.9112636 0.9152296 0.9052323 0.8935218 0.9000401 0.8869707 0.8837047 ##             V08       V09       V10 ##  [1,] 0.9223213 0.8852931 0.8929746 ##  [2,] 0.9044976 0.8865806 0.9026687 ##  [3,] 0.9139487 0.1770633 0.9222647 ##  [4,] 0.9077645 0.9109467 0.9142646 ##  [5,] 0.8797781 0.8815338 0.8907767 ##  [6,] 0.8961334 0.8931672 0.9064453 ##  [7,] 0.9032734 0.9106445 0.9160885 ##  [8,] 0.8796820 0.9247945 0.1708753 ##  [9,] 0.8968698 0.8872700 0.9004598 ## [10,] 0.1616073 0.9255980 0.8824513 ## Use lpSolve to match fitted topics to true topics dist = hellinger(phi, t(beta_mx)) soln = lp.assign(dist) soln$solution ##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ##  [1,]    0    1    0    0    0    0    0    0    0     0 ##  [2,]    0    0    0    0    1    0    0    0    0     0 ##  [3,]    0    0    0    0    0    0    0    0    1     0 ##  [4,]    0    0    1    0    0    0    0    0    0     0 ##  [5,]    0    0    0    0    0    0    1    0    0     0 ##  [6,]    0    0    0    0    0    1    0    0    0     0 ##  [7,]    1    0    0    0    0    0    0    0    0     0 ##  [8,]    0    0    0    0    0    0    0    0    0     1 ##  [9,]    0    0    0    1    0    0    0    0    0     0 ## [10,]    0    0    0    0    0    0    0    1    0     0 hellinger(phi, soln$solution %*% t(beta_mx)) ##            [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7] ##  [1,] 0.1637336 0.8751537 0.8852931 0.9165058 0.9036184 0.8995845 0.9031738 ##  [2,] 0.8784121 0.1568935 0.8865806 0.9101200 0.8912032 0.9154690 0.9059649 ##  [3,] 0.8784830 0.8950844 0.1770633 0.9078316 0.8863298 0.8955629 0.9109254 ##  [4,] 0.9175881 0.9071975 0.9109467 0.1665250 0.9121768 0.8779525 0.9364565 ##  [5,] 0.9050252 0.8866189 0.8815338 0.9027020 0.1828639 0.9016970 0.8953244 ##  [6,] 0.9117772 0.9100060 0.8931672 0.8779031 0.9014234 0.1807501 0.9229747 ##  [7,] 0.9068759 0.9020294 0.9106445 0.9378145 0.8995111 0.9310713 0.1642187 ##  [8,] 0.8876726 0.9003917 0.9247945 0.9160216 0.8869320 0.9068250 0.9146167 ##  [9,] 0.9145803 0.8991201 0.8872700 0.9014559 0.8980827 0.9032983 0.8956926 ## [10,] 0.9152296 0.9000401 0.9255980 0.9052323 0.8837047 0.8869707 0.9112636 ##            [,8]      [,9]     [,10] ##  [1,] 0.8929746 0.9123347 0.9223213 ##  [2,] 0.9026687 0.8988053 0.9044976 ##  [3,] 0.9222647 0.8766395 0.9139487 ##  [4,] 0.9142646 0.9072348 0.9077645 ##  [5,] 0.8907767 0.8977285 0.8797781 ##  [6,] 0.9064453 0.8931918 0.8961334 ##  [7,] 0.9160885 0.8921973 0.9032734 ##  [8,] 0.1708753 0.8977089 0.8796820 ##  [9,] 0.9004598 0.1682888 0.8968698 ## [10,] 0.8824513 0.8935218 0.1616073 hellinger(phi, soln$solution %*% t(beta_mx)) |>     diag() |>     summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ##  0.1569  0.1639  0.1674  0.1693  0.1755  0.1829 beta_stm_mx = tidy(fitted_stm, matrix = 'beta') |>      ## Fix order of words     mutate(term = as.integer(term)) |>     arrange(term) |>     ## And dropped words     complete(term = 1:vocab, topic, fill = list(beta = 0)) |>     pivot_wider(names_from = 'topic',                 values_from = 'beta', values_fill = 0,                 names_sort = TRUE) |>     # select(-`NA`) |>     ## Coerce to matrix     column_to_rownames('term') |>     as.matrix()  hellinger(phi, t(beta_stm_mx)) ##                1          2          3          4          5          6 ##  [1,] 0.08551476 0.84250732 0.84248134 0.88213508 0.87444870 0.87157834 ##  [2,] 0.84329922 0.85367721 0.08226114 0.87003933 0.85853111 0.86964720 ##  [3,] 0.84814338 0.08498287 0.84966302 0.85873155 0.83485457 0.87730942 ##  [4,] 0.88728074 0.86458774 0.87596766 0.08716275 0.86137846 0.90341396 ##  [5,] 0.86509433 0.84154766 0.84198078 0.86693020 0.85183981 0.85492194 ##  [6,] 0.89275208 0.85457918 0.87148785 0.83690625 0.84770470 0.89097927 ##  [7,] 0.87604529 0.87475670 0.86935797 0.89497509 0.86136575 0.08427024 ##  [8,] 0.86179446 0.88513627 0.87104990 0.87765133 0.85763362 0.88206046 ##  [9,] 0.88272319 0.84014092 0.86831204 0.85799803 0.09038475 0.86010142 ## [10,] 0.88217135 0.87848592 0.86296463 0.86880249 0.85155013 0.87344130 ##                7          8          9         10 ##  [1,] 0.85051526 0.86539658 0.87856817 0.86805865 ##  [2,] 0.86158904 0.87025988 0.85982995 0.85353582 ##  [3,] 0.88224633 0.85827133 0.87289308 0.84502101 ##  [4,] 0.87295764 0.83661054 0.86684110 0.87271693 ##  [5,] 0.84603547 0.86047913 0.83835750 0.07902295 ##  [6,] 0.86282228 0.08880347 0.85411829 0.86252234 ##  [7,] 0.87474253 0.89536072 0.86677860 0.85727623 ##  [8,] 0.09267689 0.87054310 0.84452997 0.85300612 ##  [9,] 0.85178500 0.85313348 0.85250779 0.85785724 ## [10,] 0.84600884 0.85587004 0.09097089 0.84596278 rotation_stm = hellinger(phi, t(beta_stm_mx)) |>      lp.assign() |>      magrittr::extract2('solution')  hellinger(phi, rotation_stm %*% t(beta_stm_mx)) |>     diag() |>     summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ## 0.07902 0.08445 0.08634 0.08661 0.08999 0.09268"},{"path":"https://dhicks.github.io/tmfast/articles/test 2.html","id":"topic-document-distributions","dir":"Articles","previous_headings":"","what":"Topic-document distributions","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"extract topic-document distributions using tidy() function, specifying matrix gamma including rotation align fitted true topics.4 Tile parallel coordinates plots can used visualize topic-document distributions. show varimax topic models successfully recover overall association document’s journal distinctive topic.   However, fitted topic-document distributions flatter true ones. Consider true fitted distributions document 1. Compared true distribution, fitted distribution somewhat lower probability topic V01 somewhat higher probability topics.  flatter distribution corresponds greater entropy. simulation, entropy fitted distributions 1 bit greater true distributions. discrepancy tends become worse greater values \\(k\\). mitigate problem, add optional renormalization step converting document scores topic-document distributions. Given discrete probability distribution \\(P\\) components \\(p_i\\) entropy \\(H\\), parameter \\(\\beta\\), can define new distribution \\(P'\\) components \\[ p'_i = \\frac{p_i^\\beta}{\\sum_i p_i^\\beta} = \\frac{p_i^\\beta}{Z}\\] entropy \\[ H' = \\frac{1}{Z} \\sum_i [p_i^\\beta \\beta \\log p_i] - \\log Z.\\] , can choose parameter \\(\\beta\\) renormalizes \\(P\\) achieve target entropy \\(H'\\). LDA, target entropy expected entropy topic-document distributions drawn Dirichlet prior. tmfast provides convenience functions calculating expected entropy; compare mean entropy distributions theta . actual applications, Dirichlet prior idealization, choosing \\(\\alpha\\) set target entropy important researcher degree freedom. equivalent choosing prior parameters topic modeling packages. Since solving equation \\(H'\\) \\(\\beta\\) requires numerical optimization, ’s inefficient every time call tidy(), especially large corpora. Instead, tmfast::target_power() used run optimization , return mean value across documents. use single value \\(\\beta\\) future calls tidy(). renormalized topic-document distributions closer entropy theta. keep_original argument lets us compare original renormalized distributions. can now assess accuracy topic-document distributions. used hellinger() method two matrices. method two dataframes requires specifying id, topic, probability columns. tile plot shows true fitted topics aligned (used rotation extracting gamma_df ), can get overall summary diagonal. Without renormalization, current simulation mean Hellinger distance 0.24 — bad, perhaps larger one like. larger values \\(k\\), accuracy increases significantly. Renormalization keeps mean distance around 0.13, slightly better word-topic distributions.  STM slightly closer fit, mean Hellinger distance 0.08.","code":"gamma_df = tidy(fitted, k, 'gamma',                  rotation = soln$solution) ## Warning in tidy.tmfast(fitted, k, \"gamma\", rotation = soln$solution): Rotating ## scores gamma_df |>     mutate(document = as.integer(document)) |>     ggplot(aes(document, topic, fill = gamma)) +     geom_raster() +     scale_x_continuous(breaks = NULL) gamma_df |>     mutate(document = as.integer(document),            journal = (document - 1) %/% Mj + 1) |>     ggplot(aes(topic, gamma,                 group = document, color = as.factor(journal))) +     geom_line(alpha = .25) +     facet_wrap(vars(journal), scales = 'free_x') +     scale_color_discrete(guide = 'none') ggplot(mapping = aes(topic, group = 1L)) +     geom_line(mapping = aes(y = theta, color = 'true'),                data = tibble(theta = theta[1,],                              topic = tmfast:::make_colnames(1:k))) +     geom_line(mapping = aes(y = gamma, color = 'fitted'),                data = filter(gamma_df, document == '1')) theta |>      apply(1, entropy) |>      summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ##  0.1006  0.6614  0.9715  1.0100  1.3311  2.5821 tidy(fitted, k, 'gamma') |>      group_by(document) |>      summarize(H = entropy(gamma)) |>      pull(H) |>      summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ##  0.8486  1.7040  1.9319  1.9129  2.1592  2.7673 peak_alpha(k, 1, topic_peak, topic_scale) ##  [1] 8.0000000 0.2222222 0.2222222 0.2222222 0.2222222 0.2222222 0.2222222 ##  [8] 0.2222222 0.2222222 0.2222222 expected_entropy(peak_alpha(k, 1, topic_peak, topic_scale)) ## [1] 0.997604 gamma_power = tidy(fitted, k, 'gamma') |>      target_power(document, gamma,                   expected_entropy(peak_alpha(k,                                               1,                                               topic_peak,                                               topic_scale))) gamma_power ## [1] 1.539377 gamma_df = tidy(fitted, k, 'gamma',                  rotation = soln$solution,                  exponent = gamma_power,                  keep_original = TRUE) ## Warning in tidy.tmfast(fitted, k, \"gamma\", rotation = soln$solution, exponent = ## gamma_power, : Rotating scores gamma_df |>      group_by(document) |>      summarize(across(c(gamma, gamma_rn), entropy)) |>      summarize(across(c(gamma, gamma_rn), mean)) ## # A tibble: 1 × 2 ##   gamma gamma_rn ##   <dbl>    <dbl> ## 1  1.91     1.03 ## w/o renormalization, mean distance is .24 hellinger(theta_df, doc,                         topicsdf2 = gamma_df, id2 = document,                          prob2 = gamma, df = FALSE) |>      diag() |>      summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ## 0.08499 0.20131 0.23733 0.23770 0.27244 0.37585 ## w/ renormalization, mean distance drops to .13 doc_compare = hellinger(theta_df, doc,           topicsdf2 = gamma_df, id2 = document,            prob2 = gamma_rn, df = TRUE)  doc_compare |>      filter(doc == document) |>      pull(dist) |>      summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ## 0.04808 0.10195 0.12378 0.12518 0.14564 0.24868 ggplot(doc_compare, aes(as.integer(doc),                          as.integer(document),                          fill = 1 - dist)) +     geom_raster() +     scale_x_discrete(breaks = NULL, name = 'true') +     scale_y_discrete(breaks = NULL, name = 'fitted') fitted_stm_gamma = tidy(fitted_stm, matrix = 'gamma') |>      pivot_wider(names_from = 'topic',                  values_from = 'gamma') |>      column_to_rownames('document') |>      as.matrix()  hellinger(theta, fitted_stm_gamma %*% t(rotation_stm)) |>      diag() |>      summary() ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  ## 0.03216 0.07148 0.08638 0.08823 0.10260 0.19884"},{"path":"https://dhicks.github.io/tmfast/articles/test 2.html","id":"discursive-space-visualization-using-t-sne-and-umap","dir":"Articles","previous_headings":"","what":"Discursive space visualization using t-SNE and UMAP","title":"Fitting topic models (and simulating text data) with `tmfast`","text":"Hicks (2021) proposed using topic models Hellinger distance analyze relative position documents “discursive space.” paper used t-SNE dimension-reduction algorithm produce 2D visualization Hellinger distances. UMAP algorithm also popular, believed better t-SNE preserving global structure. tmfast provides functions tsne() umap() efficiently produce visualizations. Note t-SNE algorithm involves random number draws (believe set initial positions points), repeatedly running next chunk produce superficially different visualizations.","code":"tsne(fitted, k) |>     mutate(journal = (as.integer(document)-1) %/% Mj + 1) |>     ggplot(aes(x, y, color = as.character(journal))) +     geom_point() +     labs(title = 't-SNE visualization') umap(fitted, k, df = TRUE) |>      mutate(journal = (as.integer(document)-1) %/% Mj + 1) |>     ggplot(aes(x, y, color = as.character(journal))) +     geom_point() +     labs(title = 'UMAP visualization')"},{"path":"https://dhicks.github.io/tmfast/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Dan Hicks. Author, maintainer.","code":""},{"path":"https://dhicks.github.io/tmfast/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hicks D (2023). tmfast: Fast Topic Models Using Varimax. https://dhicks.github.io/tmfast/, https://github.com/dhicks/tmfast.","code":"@Manual{,   title = {tmfast: Fast Topic Models Using Varimax},   author = {Dan Hicks},   year = {2023},   note = {https://dhicks.github.io/tmfast/, https://github.com/dhicks/tmfast}, }"},{"path":"https://dhicks.github.io/tmfast/index.html","id":"tmfast-fast-fitting-of-topic-models-using-pca--varimax","dir":"","previous_headings":"","what":"Fast Topic Models Using Varimax","title":"Fast Topic Models Using Varimax","text":"package implements approach quickly fitting topic models, combining partial PCA sparse matrices varimax rotation, proposed Rohe Zang (https://arxiv.org/abs/2004.05387). simulation, implemented method runs roughly order magnitude faster structural topic models stm package. method also deterministic introduce research degrees freedom Bayesian priors LDA. Beyond fitting topic models, package includes () functions information-theoretic approach vocabulary selection; (b) tidiers, extracting word-topic topic-document matrices tidyverse workflow; (c) Hellinger distance calculations t-SNE UMAP visualization “discursive space” analysis; (d) samplers construct simulated corpora.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/build_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a long dataframe to a wide (sparse) matrix — build_matrix","title":"Convert a long dataframe to a wide (sparse) matrix — build_matrix","text":"alias tidytext::cast_sparse","code":""},{"path":"https://dhicks.github.io/tmfast/reference/build_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a long dataframe to a wide (sparse) matrix — build_matrix","text":"","code":"build_matrix(data, row, column, value, ..., sparse = TRUE)"},{"path":"https://dhicks.github.io/tmfast/reference/build_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a long dataframe to a wide (sparse) matrix — build_matrix","text":"data Dataframe row Column name use row names, string symbol column Column name use column names, string symbol value Column name use matrix values, string symbol ... arguments, passed Matrix::sparseMatrix sparse matrix Matrix sparse matrix?","code":""},{"path":"https://dhicks.github.io/tmfast/reference/build_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a long dataframe to a wide (sparse) matrix — build_matrix","text":"matrix sparse Matrix object, one row unique value row column, one column unique value column column, many non-zero values rows data.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/build_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a long dataframe to a wide (sparse) matrix — build_matrix","text":"","code":"data.frame(id = c(1, 1, 2, 2) + 4,            cols = c('a', 'b', 'a', 'b'),            vals = 1:4) |>     build_matrix(row = id, column = 'cols', value = vals) #> 2 x 2 sparse Matrix of class \"dgCMatrix\" #>   a b #> 5 1 2 #> 6 3 4"},{"path":"https://dhicks.github.io/tmfast/reference/draw_a_word.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw a single word given topic and word distributions — draw_a_word","title":"Draw a single word given topic and word distributions — draw_a_word","text":"Draw single word given topic word distributions","code":""},{"path":"https://dhicks.github.io/tmfast/reference/draw_a_word.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw a single word given topic and word distributions — draw_a_word","text":"","code":"draw_a_word(theta, phi)"},{"path":"https://dhicks.github.io/tmfast/reference/draw_a_word.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw a single word given topic and word distributions — draw_a_word","text":"theta Topic distribution, length \\(k\\) vector phi Word distribution topics, \\(k \\times v\\) matrix","code":""},{"path":"https://dhicks.github.io/tmfast/reference/draw_a_word.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draw a single word given topic and word distributions — draw_a_word","text":"Integer representation single word","code":""},{"path":"https://dhicks.github.io/tmfast/reference/draw_corpus.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw a collection of documents — draw_corpus","title":"Draw a collection of documents — draw_corpus","text":"Draw collection documents","code":""},{"path":"https://dhicks.github.io/tmfast/reference/draw_corpus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw a collection of documents — draw_corpus","text":"","code":"draw_corpus(N, theta, phi)"},{"path":"https://dhicks.github.io/tmfast/reference/draw_corpus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw a collection of documents — draw_corpus","text":"N Length documents theta Topic distribution documents, \\(n \\times k\\) matrix phi Word distribution topics, \\(k \\times v\\) matrix","code":""},{"path":"https://dhicks.github.io/tmfast/reference/draw_corpus.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draw a collection of documents — draw_corpus","text":"Document-term matrix, tibble, columns doc, word, n","code":""},{"path":[]},{"path":"https://dhicks.github.io/tmfast/reference/draw_words.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw words for one document — draw_words","title":"Draw words for one document — draw_words","text":"Draw words one document","code":""},{"path":"https://dhicks.github.io/tmfast/reference/draw_words.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw words for one document — draw_words","text":"","code":"draw_words(Ni, theta, phi)"},{"path":"https://dhicks.github.io/tmfast/reference/draw_words.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw words for one document — draw_words","text":"Ni length document theta theta_i, topic distribution document phi word distribution topics","code":""},{"path":"https://dhicks.github.io/tmfast/reference/draw_words.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draw words for one document — draw_words","text":"Document-term matrix (single document), tibble, columns word n","code":""},{"path":"https://dhicks.github.io/tmfast/reference/entropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Entropy of a distribution — entropy","title":"Entropy of a distribution — entropy","text":"Entropy distribution","code":""},{"path":"https://dhicks.github.io/tmfast/reference/entropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Entropy of a distribution — entropy","text":"","code":"entropy(p, base = 2)"},{"path":"https://dhicks.github.io/tmfast/reference/entropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Entropy of a distribution — entropy","text":"p Discrete probability distribution base Desired base entropy, eg, 2 bits","code":""},{"path":"https://dhicks.github.io/tmfast/reference/entropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Entropy of a distribution — entropy","text":"Calculated Shannon entropy","code":""},{"path":"https://dhicks.github.io/tmfast/reference/expected_entropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Expected entropy for samples from a Dirichlet distribution — expected_entropy","title":"Expected entropy for samples from a Dirichlet distribution — expected_entropy","text":"Samples P = <p1, p2, ..., pk> Dirichlet distribution parameter alpha = <alpha1, alpha2, ..., alphak> can treated categorical probability distributions entropy H(P) = sum(-p log(p)).  function calculates expected entropy EH(P) given alpha.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/expected_entropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expected entropy for samples from a Dirichlet distribution — expected_entropy","text":"","code":"expected_entropy(alpha, k = NULL)"},{"path":"https://dhicks.github.io/tmfast/reference/expected_entropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expected entropy for samples from a Dirichlet distribution — expected_entropy","text":"alpha Dirichlet parameter k length(alpha) 1, number components symmetric Dirichlet distribution","code":""},{"path":"https://dhicks.github.io/tmfast/reference/expected_entropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expected entropy for samples from a Dirichlet distribution — expected_entropy","text":"Expected entropy EH(P) bits (log2 scale)","code":""},{"path":"https://dhicks.github.io/tmfast/reference/expected_entropy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Expected entropy for samples from a Dirichlet distribution — expected_entropy","text":"https://math.stackexchange.com/questions/2266285/expected-entropy-based--dirichlet-distribution/3195376#3195376","code":""},{"path":"https://dhicks.github.io/tmfast/reference/expected_entropy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expected entropy for samples from a Dirichlet distribution — expected_entropy","text":"","code":"alpha = peak_alpha(50, 1) set.seed(1357) rdirichlet(500, alpha) |>   apply(1, entropy) |>   mean() #> [1] 0.4568703 expected_entropy(alpha) #> [1] 0.445642"},{"path":"https://dhicks.github.io/tmfast/reference/fit_varimax.html","id":null,"dir":"Reference","previous_headings":"","what":"Given a (rank n) PCA fit, return a rank k < n varimax fit — fit_varimax","title":"Given a (rank n) PCA fit, return a rank k < n varimax fit — fit_varimax","text":"Given (rank n) PCA fit, return rank k < n varimax fit","code":""},{"path":"https://dhicks.github.io/tmfast/reference/fit_varimax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Given a (rank n) PCA fit, return a rank k < n varimax fit — fit_varimax","text":"","code":"fit_varimax(   k,   pca,   feature_names,   obs_names,   varimax_fn = stats::varimax,   varimax_opts = NULL,   positive_skew = TRUE,   x = NULL )"},{"path":"https://dhicks.github.io/tmfast/reference/fit_varimax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Given a (rank n) PCA fit, return a rank k < n varimax fit — fit_varimax","text":"k Desired rank fitted varimax model pca Fitted PCA model feature_names Names features (eg, data columns) obs_names Names observations (eg, data rows) varimax_fn Function use varimax rotation varimax_opts Options passed varimax_fn positive_skew negative-skewed factors flipped positive skew? x Original data matrix; passed included pca (eg, via retx = TRUE)","code":""},{"path":"https://dhicks.github.io/tmfast/reference/fit_varimax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Given a (rank n) PCA fit, return a rank k < n varimax fit — fit_varimax","text":"List components - loadings: Rotated feature loadings - rotmat:  Rotation matrix - scores:  Rotated observation scores","code":""},{"path":"https://dhicks.github.io/tmfast/reference/fit_varimax.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Given a (rank n) PCA fit, return a rank k < n varimax fit — fit_varimax","text":"initial rotation, factors negative skew (left tails) flipped","code":""},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.Matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Hellinger distance for matrices — hellinger.Matrix","title":"Hellinger distance for matrices — hellinger.Matrix","text":"Calculates Hellinger distance pair rows given matrix, combination rows two matrices","code":""},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.Matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hellinger distance for matrices — hellinger.Matrix","text":"","code":"# S3 method for Matrix hellinger(mx1, mx2 = NULL)"},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.Matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hellinger distance for matrices — hellinger.Matrix","text":"mx1 First matrix, \\(n_1 \\times k\\) mx2 Optional second matrix, \\(n_2 \\times k\\)","code":""},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.Matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hellinger distance for matrices — hellinger.Matrix","text":"Matrix size \\(n_1 \\times n_1\\) \\(n_1 \\times n_2\\)","code":""},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.Matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hellinger distance for matrices — hellinger.Matrix","text":"","code":"set.seed(2022-06-09) mx1 = rdirichlet(3, rep(5, 5)) mx2 = rdirichlet(3, rep(5, 5)) hellinger(mx1) #>              [,1]      [,2]      [,3] #> [1,] 1.053671e-08 0.3067419 0.2668745 #> [2,] 3.067419e-01 0.0000000 0.1230902 #> [3,] 2.668745e-01 0.1230902 0.0000000 hellinger(mx1, mx2) #>           [,1]      [,2]       [,3] #> [1,] 0.2361547 0.2632094 0.33018266 #> [2,] 0.1777705 0.1060308 0.12270871 #> [3,] 0.1296687 0.1732766 0.08788004"},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.data.frame.html","id":null,"dir":"Reference","previous_headings":"","what":"Hellinger distance for dataframes — hellinger.data.frame","title":"Hellinger distance for dataframes — hellinger.data.frame","text":"Hellinger distances, either pairwise within single tidied topic model dataframe two tidied topic model dataframes","code":""},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.data.frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hellinger distance for dataframes — hellinger.data.frame","text":"","code":"# S3 method for data.frame hellinger(   topicsdf1,   id1 = \"document\",   cat1 = \"topic\",   prob1 = \"prob\",   topicsdf2 = NULL,   id2 = \"document\",   cat2 = \"topic\",   prob2 = \"prob\",   df = FALSE )"},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.data.frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hellinger distance for dataframes — hellinger.data.frame","text":"df function return matrix Hellinger distances (default) tidy dataframe? topicsdf1, topicsdf2 Tidied topic model dataframes id1, id2 Unit identifiers (DOIs, auids, ORU name, etc.) cat1, cat2 Category identifiers (topics) prob1, prob2 Probability values (gamma)","code":""},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.data.frame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hellinger distance for dataframes — hellinger.data.frame","text":"matrix tidy dataframe (default) Hellinger distances","code":""},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.data.frame.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hellinger distance for dataframes — hellinger.data.frame","text":"","code":"set.seed(2022-06-09) topics1 = rdirichlet(3, rep(5, 5)) |>     tibble::as_tibble(rownames = 'doc_id',                       .name_repair = tmfast:::make_colnames) |>     dplyr::mutate(doc_id = stringr::str_c('doc_', doc_id)) |>     tidyr::pivot_longer(tidyselect::starts_with('V'),                         names_to = 'topic',                         values_to = 'gamma') topics2 = rdirichlet(3, rep(5, 5)) |>     tibble::as_tibble(rownames = 'doc_id',                       .name_repair = tmfast:::make_colnames) |>     dplyr::mutate(doc_id = stringr::str_c('doc_', as.integer(doc_id) + 5)) |>     tidyr::pivot_longer(tidyselect::starts_with('V'),                         names_to = 'topic',                         values_to = 'gamma') hellinger(topics1, doc_id, prob1 = 'gamma', df = TRUE) #> # A tibble: 9 × 3 #>   doc_id document         dist #>   <chr>  <chr>           <dbl> #> 1 doc_1  doc_1    0.0000000105 #> 2 doc_1  doc_2    0.307        #> 3 doc_1  doc_3    0.267        #> 4 doc_2  doc_1    0.307        #> 5 doc_2  doc_2    0            #> 6 doc_2  doc_3    0.123        #> 7 doc_3  doc_1    0.267        #> 8 doc_3  doc_2    0.123        #> 9 doc_3  doc_3    0            hellinger(topics1, doc_id, prob1 = 'gamma',           topicsdf2 = topics2, id2 = doc_id, prob2 = 'gamma') #>           doc_6     doc_7      doc_8 #> doc_1 0.2361547 0.2632094 0.33018266 #> doc_2 0.1777705 0.1060308 0.12270871 #> doc_3 0.1296687 0.1732766 0.08788004"},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.html","id":null,"dir":"Reference","previous_headings":"","what":"Hellinger distances — hellinger","title":"Hellinger distances — hellinger","text":"Hellinger distances","code":""},{"path":"https://dhicks.github.io/tmfast/reference/hellinger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hellinger distances — hellinger","text":"","code":"hellinger(x, ...)"},{"path":"https://dhicks.github.io/tmfast/reference/insert_topics.html","id":null,"dir":"Reference","previous_headings":"","what":"Insert a topic model into a fitted tmfast — insert_topics","title":"Insert a topic model into a fitted tmfast — insert_topics","text":"Apply varimax rotation value k less maximum already included tmfast.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/insert_topics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Insert a topic model into a fitted tmfast — insert_topics","text":"","code":"insert_topics(fitted, k, x = NULL)"},{"path":"https://dhicks.github.io/tmfast/reference/insert_topics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Insert a topic model into a fitted tmfast — insert_topics","text":"fitted Fitted tmfast object k Desired number topics new model x Data matrix (document-term matrix), Matrix object (eg, using build_matrix())","code":""},{"path":"https://dhicks.github.io/tmfast/reference/insert_topics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Insert a topic model into a fitted tmfast — insert_topics","text":"tmfast object, fitted, additional topic model inserted","code":""},{"path":"https://dhicks.github.io/tmfast/reference/journal_specific.html","id":null,"dir":"Reference","previous_headings":"","what":"","title":"","text":"Generates corpus Mj documents k journals, characteristic topic.  Fits varimax topic model rank k, rotates word-topic distribution align true values, reports Hellinger distance comparisons topic (word-topic) document (topic-doc).","code":""},{"path":"https://dhicks.github.io/tmfast/reference/journal_specific.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"","text":"","code":"journal_specific(   k = 5,   Mj = 100,   topic_peak = 0.8,   topic_scale = 10,   word_beta = 0.01,   vocab = 10 * Mj * k,   size = 3,   mu = 300,   bigjournal = FALSE,   verbose = TRUE )"},{"path":"https://dhicks.github.io/tmfast/reference/journal_specific.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"","text":"k Number topics/journals Mj Number documents journal word_beta Parameter symmetric Dirichlet prior true word-doc distributions vocab Size vocabulary bigjournal first journal documents 10x long (average) others? verbose TRUE, sends messages progress simulation topic_peak, topic_scale Parameters asymmetric Dirichlet prior true topic-doc distributions size, mu Parameters negative binomial distribution document lengths","code":""},{"path":[]},{"path":"https://dhicks.github.io/tmfast/reference/loadings.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract a PCA/varimax loadings matrix — loadings","title":"Extract a PCA/varimax loadings matrix — loadings","text":"Extract PCA/varimax loadings matrix","code":""},{"path":"https://dhicks.github.io/tmfast/reference/loadings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract a PCA/varimax loadings matrix — loadings","text":"","code":"loadings(x, ...)"},{"path":"https://dhicks.github.io/tmfast/reference/make_colnames.html","id":null,"dir":"Reference","previous_headings":"","what":"Make colnames — make_colnames","title":"Make colnames — make_colnames","text":"Helper function make matrix column names form 'V09'","code":""},{"path":"https://dhicks.github.io/tmfast/reference/make_colnames.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make colnames — make_colnames","text":"","code":"make_colnames(names, prefix = \"V\")"},{"path":"https://dhicks.github.io/tmfast/reference/ndH.html","id":null,"dir":"Reference","previous_headings":"","what":"Information gain (uniform distribution) — ndH","title":"Information gain (uniform distribution) — ndH","text":"Calculates \\(\\log_2 n \\times \\delta H\\), log total occurrence times information gain (relative uniform distribution) term. prefer vocabulary selection methods TF-IDF.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/ndH.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Information gain (uniform distribution) — ndH","text":"","code":"ndH(dataf, doc_col, term_col, count_col)"},{"path":"https://dhicks.github.io/tmfast/reference/ndH.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Information gain (uniform distribution) — ndH","text":"dataf Tidy document-term matrix doc_col Column dataf document IDs term_col Column dataf terms count_col Column dataf document-term counts","code":""},{"path":"https://dhicks.github.io/tmfast/reference/ndH.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Information gain (uniform distribution) — ndH","text":"Dataframe columns","code":"- `{{ term col }}`, term - `dH`, information gain relative to uniform distribution over documents - `n`, total count of term occurrence - `ndH`, \\eqn{\\log_2 n \\times \\delta H}"},{"path":"https://dhicks.github.io/tmfast/reference/ndH.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Information gain (uniform distribution) — ndH","text":"","code":"library(tidyverse) #> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── #> ✔ dplyr     1.1.2     ✔ readr     2.1.4 #> ✔ forcats   1.0.0     ✔ stringr   1.5.0 #> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1 #> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0 #> ✔ purrr     1.0.2      #> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #> ✖ dplyr::filter() masks stats::filter() #> ✖ dplyr::lag()    masks stats::lag() #> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors library(tidytext) library(janeaustenr) austen_df = austen_books() |>     unnest_tokens(term, text, token = 'words') |>     mutate(author = 'Jane Austen') |>     count(author, book, term) ndH(austen_df, book, term, n) #> # A tibble: 14,520 × 5 #>    term           H    dH     n   ndH #>    <chr>      <dbl> <dbl> <int> <dbl> #>  1 emma      0.0141  2.57   787  24.7 #>  2 elinor    0       2.58   623  24.0 #>  3 crawford  0       2.58   493  23.1 #>  4 marianne  0       2.58   492  23.1 #>  5 weston    0       2.58   389  22.2 #>  6 darcy     0       2.58   373  22.1 #>  7 fanny     0.323   2.26   862  22.1 #>  8 edmund    0       2.58   364  22.0 #>  9 knightley 0       2.58   356  21.9 #> 10 harriet   0.0873  2.50   419  21.8 #> # ℹ 14,510 more rows"},{"path":"https://dhicks.github.io/tmfast/reference/ndR.html","id":null,"dir":"Reference","previous_headings":"","what":"Information gain (length-proportional distribution) — ndR","title":"Information gain (length-proportional distribution) — ndR","text":"alternative ndH() uses information gain relative distribution documents proportional length.  uniform distribution dramatic differences document lengths (eg, orders magnitude), high-ndH terms tend distinctive terms long documents.  length-proportional distribution, high information-gain terms likely come shorter documents. Informal testing suggests approach performs better ndH() uniform distribution documents widely varying lengths, eg, orders magnitude.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/ndR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Information gain (length-proportional distribution) — ndR","text":"","code":"ndR(dataf, doc_col, term_col, count_col)"},{"path":"https://dhicks.github.io/tmfast/reference/ndR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Information gain (length-proportional distribution) — ndR","text":"dataf Tidy document-term matrix doc_col Column dataf document IDs term_col Column dataf terms count_col Column dataf document-term counts","code":""},{"path":"https://dhicks.github.io/tmfast/reference/ndR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Information gain (length-proportional distribution) — ndR","text":"Dataframe columns","code":"- `{{ term col }}`, term - `n`, total count of term occurrence - `dR`, information gain relative to length-proportional distribution over documents - `ndR`, \\eqn{\\log_2 n \\times \\delta R}"},{"path":"https://dhicks.github.io/tmfast/reference/ndR.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Information gain (length-proportional distribution) — ndR","text":"","code":"library(tidyverse) library(tidytext) library(janeaustenr) austen_df = austen_books() |>     unnest_tokens(term, text, token = 'words') |>     mutate(author = 'Jane Austen') |>     count(author, book, term) ndR(austen_df, book, term, n) #> # A tibble: 14,520 × 4 #>    term          n    dR   ndR #>    <chr>     <int> <dbl> <dbl> #>  1 elliot      254  3.12  24.9 #>  2 tilney      196  3.22  24.5 #>  3 anne        467  2.76  24.5 #>  4 elinor      623  2.60  24.1 #>  5 wentworth   191  3.12  23.6 #>  6 marianne    492  2.60  23.2 #>  7 thorpe      126  3.22  22.5 #>  8 morland     125  3.22  22.4 #>  9 allen       116  3.22  22.1 #> 10 darcy       373  2.57  21.9 #> # ℹ 14,510 more rows"},{"path":"https://dhicks.github.io/tmfast/reference/peak_alpha.html","id":null,"dir":"Reference","previous_headings":"","what":"Alpha parameter with a single peak — peak_alpha","title":"Alpha parameter with a single peak — peak_alpha","text":"function allows us quickly define alpha parameter Dirichlet distribution single (presumably high) peak*scale value component components uniform (presumably low) value (1-peak)/(k-1)*scale.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/peak_alpha.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Alpha parameter with a single peak — peak_alpha","text":"","code":"peak_alpha(k, i, peak = 0.8, scale = 1)"},{"path":"https://dhicks.github.io/tmfast/reference/peak_alpha.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Alpha parameter with a single peak — peak_alpha","text":"k Number components Index component takes value peak peak Value single peak component","code":""},{"path":"https://dhicks.github.io/tmfast/reference/peak_alpha.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Alpha parameter with a single peak — peak_alpha","text":"Vector length k","code":""},{"path":[]},{"path":"https://dhicks.github.io/tmfast/reference/rdirichlet.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from the Dirichlet distribution — rdirichlet","title":"Sample from the Dirichlet distribution — rdirichlet","text":"Sample Dirichlet distribution","code":""},{"path":"https://dhicks.github.io/tmfast/reference/rdirichlet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from the Dirichlet distribution — rdirichlet","text":"","code":"rdirichlet(n, alpha, k = NULL)"},{"path":"https://dhicks.github.io/tmfast/reference/rdirichlet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from the Dirichlet distribution — rdirichlet","text":"n Number samples (rows) draw alpha Concentration parameters; either length 1 length > 1 length 1, assumes symmetric Dirichlet; k must null k Number components (columns); ignored length(alpha) > 1","code":""},{"path":"https://dhicks.github.io/tmfast/reference/rdirichlet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample from the Dirichlet distribution — rdirichlet","text":"matrix n rows length(alpha) k columns","code":""},{"path":[]},{"path":"https://dhicks.github.io/tmfast/reference/rdirichlet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from the Dirichlet distribution — rdirichlet","text":"","code":"rdirichlet(10, .1, 5) #>               [,1]         [,2]         [,3]         [,4]         [,5] #>  [1,] 2.423841e-09 1.532803e-12 2.169466e-06 7.406266e-01 2.593712e-01 #>  [2,] 1.459912e-07 9.978821e-01 2.041467e-03 7.548776e-05 8.241854e-07 #>  [3,] 6.691078e-02 8.739031e-01 8.838151e-08 1.322609e-13 5.918602e-02 #>  [4,] 9.997311e-01 2.344897e-04 9.438505e-06 2.016269e-05 4.762268e-06 #>  [5,] 5.705727e-01 3.289170e-03 3.885484e-03 1.422979e-01 2.799548e-01 #>  [6,] 3.979995e-11 1.145891e-04 7.392265e-02 3.694382e-07 9.259624e-01 #>  [7,] 3.620254e-07 7.164644e-09 2.523535e-07 9.277864e-01 7.221295e-02 #>  [8,] 2.877197e-01 6.969973e-01 8.160363e-04 3.396563e-03 1.107040e-02 #>  [9,] 2.667917e-09 2.071258e-03 9.659243e-01 1.175474e-02 2.024974e-02 #> [10,] 3.164987e-12 1.471874e-01 3.901119e-03 2.830459e-02 8.206069e-01   rdirichlet(10, c(.8, .1, .1)) #>             [,1]         [,2]         [,3] #>  [1,] 0.99233959 4.493176e-08 7.660367e-03 #>  [2,] 0.81257946 1.874204e-01 9.633943e-08 #>  [3,] 0.99995797 4.202902e-05 2.243478e-16 #>  [4,] 0.86415746 1.231873e-01 1.265520e-02 #>  [5,] 0.05493031 6.767110e-01 2.683587e-01 #>  [6,] 0.14377540 8.483355e-01 7.889068e-03 #>  [7,] 0.97931985 1.905010e-02 1.630049e-03 #>  [8,] 0.27276990 4.473032e-02 6.824998e-01 #>  [9,] 0.85135935 1.486406e-01 5.826725e-09 #> [10,] 0.96300713 8.924995e-03 2.806787e-02"},{"path":"https://dhicks.github.io/tmfast/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. generics tidy","code":""},{"path":"https://dhicks.github.io/tmfast/reference/renorm.html","id":null,"dir":"Reference","previous_headings":"","what":"Renormalize tidied distributions — renorm","title":"Renormalize tidied distributions — renorm","text":"Given tidied dataframe topic-doc word-topic distributions exponent, renormalizes distributions.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/renorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Renormalize tidied distributions — renorm","text":"","code":"renorm(tidy_df, group_col, p_col, exponent, keep_original = FALSE)"},{"path":"https://dhicks.github.io/tmfast/reference/renorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Renormalize tidied distributions — renorm","text":"tidy_df tidied distribution dataframe group_col Grouping column, RHS conditional probability distribution, eg, topics word-topic distributions p_col Column containing probability category (eg, word) conditional group (eg, topic) exponent Exponent use renormalization keep_original Keep original probabilities?","code":""},{"path":"https://dhicks.github.io/tmfast/reference/renorm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Renormalize tidied distributions — renorm","text":"dataframe (keep_original TRUE) added column form p_col_rn containing renormalized probabilities (keep_original FALSE) renormalized values p_col.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/rotation.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract varimax rotation — rotation","title":"Extract varimax rotation — rotation","text":"Extract varimax rotation","code":""},{"path":"https://dhicks.github.io/tmfast/reference/rotation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract varimax rotation — rotation","text":"","code":"rotation(x, ...)"},{"path":"https://dhicks.github.io/tmfast/reference/scores.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract item scores from a fitted PCA/varimax model — scores","title":"Extract item scores from a fitted PCA/varimax model — scores","text":"Extract item scores fitted PCA/varimax model","code":""},{"path":"https://dhicks.github.io/tmfast/reference/scores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract item scores from a fitted PCA/varimax model — scores","text":"","code":"scores(x, ...)"},{"path":"https://dhicks.github.io/tmfast/reference/solve_power.html","id":null,"dir":"Reference","previous_headings":"","what":"Solve the equation to find the desired exponent — solve_power","title":"Solve the equation to find the desired exponent — solve_power","text":"https://stats.stackexchange.com/questions/521582/controlling--entropy---distribution","code":""},{"path":"https://dhicks.github.io/tmfast/reference/solve_power.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Solve the equation to find the desired exponent — solve_power","text":"","code":"solve_power(p, target_H, return_full = FALSE)"},{"path":"https://dhicks.github.io/tmfast/reference/solve_power.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Solve the equation to find the desired exponent — solve_power","text":"p Initial distribution target_H Desired entropy transformed distribution return_full Return full uniroot() output? interval Range exponents within search","code":""},{"path":"https://dhicks.github.io/tmfast/reference/solve_power.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Solve the equation to find the desired exponent — solve_power","text":"Numeric value desired exponent","code":""},{"path":"https://dhicks.github.io/tmfast/reference/target_power.html","id":null,"dir":"Reference","previous_headings":"","what":"Find target power for renormalization — target_power","title":"Find target power for renormalization — target_power","text":"Given tidied dataframe topic-doc word-topic distributions target entropy, find mean exponent needed adjust temperature distribution approximately match target entropy.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/target_power.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find target power for renormalization — target_power","text":"","code":"target_power(tidy_df, group_col, p_col, target_entropy)"},{"path":"https://dhicks.github.io/tmfast/reference/target_power.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find target power for renormalization — target_power","text":"tidy_df tidied distribution dataframe p_col Column containing probability category (eg, word) conditional group (eg, topic) target_entropy Target entropy Grouping column, RHS conditional probability distribution, eg, topics word-topic distributions","code":""},{"path":"https://dhicks.github.io/tmfast/reference/target_power.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find target power for renormalization — target_power","text":"Mean exponent renormalize target entropy","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tidy.tmfast.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract beta and gamma matrices from tmfast objects — tidy.tmfast","title":"Extract beta and gamma matrices from tmfast objects — tidy.tmfast","text":"Extract beta gamma matrices tmfast objects","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tidy.tmfast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract beta and gamma matrices from tmfast objects — tidy.tmfast","text":"","code":"# S3 method for tmfast tidy(   x,   k,   matrix = \"beta\",   df = TRUE,   exponent = NULL,   keep_original = FALSE,   rotation = NULL )"},{"path":"https://dhicks.github.io/tmfast/reference/tidy.tmfast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract beta and gamma matrices from tmfast objects — tidy.tmfast","text":"x tmfast object k Index (number topics/factors) matrix Desired matrix, either word-topic (beta) topic-doc distributions (gamma) df Return long dataframe (default) wide matrix? exponent Renormalize probabilities using given exponent  Applies df == TRUE keep_original renormalizing, return original (pre-renormalized) probabilities? rotation Optional rotation matrix; see details","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tidy.tmfast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract beta and gamma matrices from tmfast objects — tidy.tmfast","text":"long dataframe, one row per word-topic topic-doc combination. Column names depend value matrix.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tidy.tmfast.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract beta and gamma matrices from tmfast objects — tidy.tmfast","text":"rotation NULL, loadings/scores rotated.  might used align fitted topics known true topics, journal_specific simulation.  Loadings left-multiplied given rotation, scores right-multiplied transpose given rotation.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tidy_all.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract gamma or beta matrices for all topics — tidy_all","title":"Extract gamma or beta matrices for all topics — tidy_all","text":"Extract gamma beta matrices topics","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tidy_all.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract gamma or beta matrices for all topics — tidy_all","text":"","code":"tidy_all(x, matrix = \"beta\", ...)"},{"path":"https://dhicks.github.io/tmfast/reference/tidy_all.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract gamma or beta matrices for all topics — tidy_all","text":"x tmfast object matrix Desired matrix, 'beta' 'gamma' ... arguments, passed tidy.tmfast()","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tidy_all.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract gamma or beta matrices for all topics — tidy_all","text":"long dataframe, one row per word-topic topic-doc combination. Column names depend value matrix.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tmfast.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a topic model using PCA+varimax — tmfast","title":"Fit a topic model using PCA+varimax — tmfast","text":"Fit topic model using PCA+varimax","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tmfast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a topic model using PCA+varimax — tmfast","text":"","code":"tmfast(dtm, n, row = doc, column = word, value = n, verbose = FALSE, ...)"},{"path":"https://dhicks.github.io/tmfast/reference/tmfast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a topic model using PCA+varimax — tmfast","text":"dtm Document-term matrix.  Either object inheriting Matrix long dataframe representation row column row, column column column, value column n. n Number topics return row dataframe dtm, row column column dataframe dtm, column column value dataframe dtm, value column verbose irlba() verbose? ... arguments, passed varimax_irlba","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tmfast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a topic model using PCA+varimax — tmfast","text":"per varimax_irlba, class tmfast","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tmfast.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a topic model using PCA+varimax — tmfast","text":"dtm matrix, cast sparse matrix using tidytext::case_sparse()","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tsne.data.frame.html","id":null,"dir":"Reference","previous_headings":"","what":"Discursive space using t-SNE — tsne.data.frame","title":"Discursive space using t-SNE — tsne.data.frame","text":"2-dimensional \"discursive space\" representation relationships documents using Hellinger distances t-SNE","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tsne.data.frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discursive space using t-SNE — tsne.data.frame","text":"","code":"# S3 method for data.frame tsne(gamma_df, k, doc_ids, perplexity = NULL, df = TRUE)"},{"path":"https://dhicks.github.io/tmfast/reference/tsne.data.frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Discursive space using t-SNE — tsne.data.frame","text":"k Number topics (required tmfast objects) doc_ids Vector document IDs (required STM objects) perplexity Perplexity parameter t-SNE. default, minimum 30 floor((ndocs - 1)/3) - 1. df Return dataframe columns document, x, y (default) output Rtsne. tm fitted topic model","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tsne.data.frame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Discursive space using t-SNE — tsne.data.frame","text":"See df","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tsne.data.frame.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Discursive space using t-SNE — tsne.data.frame","text":"Algorithm checks distances 3*perplexity nearest neighbors.  Rtsne loses rownames (document IDs); either extract tmfast object passed separately STMobject.  default method (exported) takes tidied gamma (document-topic-gamma) matrix.  Use set.seed() calling function reproducibility.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tsne.data.frame.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Discursive space using t-SNE — tsne.data.frame","text":"","code":"## From the real books vignette set.seed(42) tsne(fitted_tmf, k = 4, df = TRUE) |>     left_join(meta, by = c('document' = 'book')) |>     ggplot(aes(x, y, color = author)) +     geom_point() #> Error in eval(expr, envir, enclos): object 'fitted_tmf' not found"},{"path":"https://dhicks.github.io/tmfast/reference/tsne.html","id":null,"dir":"Reference","previous_headings":"","what":"Discursive space using t-SNE — tsne","title":"Discursive space using t-SNE — tsne","text":"Discursive space using t-SNE","code":""},{"path":"https://dhicks.github.io/tmfast/reference/tsne.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discursive space using t-SNE — tsne","text":"","code":"tsne(x, ...)"},{"path":"https://dhicks.github.io/tmfast/reference/umap.STM.html","id":null,"dir":"Reference","previous_headings":"","what":"Discursive space with UMAP for structural topic models — umap.STM","title":"Discursive space with UMAP for structural topic models — umap.STM","text":"Discursive space UMAP structural topic models","code":""},{"path":"https://dhicks.github.io/tmfast/reference/umap.STM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discursive space with UMAP for structural topic models — umap.STM","text":"","code":"# S3 method for STM umap(tm, doc_ids, ...)"},{"path":"https://dhicks.github.io/tmfast/reference/umap.STM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Discursive space with UMAP for structural topic models — umap.STM","text":"tm STM object doc_ids Character vector document IDs ... arguments, passed umap.matrix()","code":""},{"path":"https://dhicks.github.io/tmfast/reference/umap.STM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Discursive space with UMAP for structural topic models — umap.STM","text":"umap object tidied dataframe; see umap.matrix() argument df","code":""},{"path":"https://dhicks.github.io/tmfast/reference/umap.html","id":null,"dir":"Reference","previous_headings":"","what":"Discursive space using UMAP — umap","title":"Discursive space using UMAP — umap","text":"2-dimensional \"discursive space\" representation relationships documents using Hellinger distances UMAP","code":""},{"path":"https://dhicks.github.io/tmfast/reference/umap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discursive space using UMAP — umap","text":"","code":"umap(x, ...)"},{"path":"https://dhicks.github.io/tmfast/reference/umap.matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Discursive space with UMAP given a distance matrix — umap.matrix","title":"Discursive space with UMAP given a distance matrix — umap.matrix","text":"Construct 2-dimensional \"discursive space\" embedding given distance matrix.","code":""},{"path":"https://dhicks.github.io/tmfast/reference/umap.matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discursive space with UMAP given a distance matrix — umap.matrix","text":"","code":"# S3 method for matrix umap(dist_mx, include_data = FALSE, df = TRUE, ...)"},{"path":"https://dhicks.github.io/tmfast/reference/umap.matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Discursive space with UMAP given a distance matrix — umap.matrix","text":"dist_mx Distance matrix include_data default, save space data (distance matrix) returned df Return tibble columns document, x, y (default) output umap. ... parameters passed umap::umap()","code":""},{"path":"https://dhicks.github.io/tmfast/reference/umap.matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Discursive space with UMAP given a distance matrix — umap.matrix","text":"Object class umap, components layout (coordinates items), knn (k-nearest neighbors matrices), config (UMAP configuration) tidied dataframe, per argument df","code":""},{"path":"https://dhicks.github.io/tmfast/reference/umap.matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Discursive space with UMAP given a distance matrix — umap.matrix","text":"","code":"gamma = rdirichlet(26, 1, 5) rownames(gamma) = letters h_gamma = hellinger(gamma) embedded = umap(h_gamma, df = TRUE, verbose = TRUE) #> [2023-11-15 08:24:50.33582]  starting umap #> [2023-11-15 08:24:50.369645]  creating graph of nearest neighbors #> [2023-11-15 08:24:50.388917]  creating initial embedding #> [2023-11-15 08:24:50.398499]  optimizing embedding #> [2023-11-15 08:24:50.513047]  done"},{"path":"https://dhicks.github.io/tmfast/reference/umap.tmfast.html","id":null,"dir":"Reference","previous_headings":"","what":"Discursive space with UMAP for tmfast topic models — umap.tmfast","title":"Discursive space with UMAP for tmfast topic models — umap.tmfast","text":"Construct 2-dimensional \"discursive space\" embedding given tmfast topic model","code":""},{"path":"https://dhicks.github.io/tmfast/reference/umap.tmfast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discursive space with UMAP for tmfast topic models — umap.tmfast","text":"","code":"# S3 method for tmfast umap(model, k, ...)"},{"path":"https://dhicks.github.io/tmfast/reference/umap.tmfast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Discursive space with UMAP for tmfast topic models — umap.tmfast","text":"model tmfast object k Number topics ... arguments, passed umap.matrix()","code":""},{"path":"https://dhicks.github.io/tmfast/reference/umap.tmfast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Discursive space with UMAP for tmfast topic models — umap.tmfast","text":"umap object tidied dataframe; see umap.matrix() argument df","code":""},{"path":"https://dhicks.github.io/tmfast/reference/umap.tmfast.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Discursive space with UMAP for tmfast topic models — umap.tmfast","text":"","code":"umap(fitted, 10, verbose = TRUE) #> Error in UseMethod(\"umap\"): no applicable method for 'umap' applied to an object of class \"function\""},{"path":"https://dhicks.github.io/tmfast/reference/varimax_irlba.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a varimax-rotated PCA using irlba — varimax_irlba","title":"Fit a varimax-rotated PCA using irlba — varimax_irlba","text":"Extract n principal components matrix mx using irlba, rotate solution using varimax","code":""},{"path":"https://dhicks.github.io/tmfast/reference/varimax_irlba.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a varimax-rotated PCA using irlba — varimax_irlba","text":"","code":"varimax_irlba(   mx,   n,   prcomp_fn = irlba::prcomp_irlba,   prcomp_opts = NULL,   varimax_fn = stats::varimax,   varimax_opts = NULL,   retx = FALSE )"},{"path":"https://dhicks.github.io/tmfast/reference/varimax_irlba.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a varimax-rotated PCA using irlba — varimax_irlba","text":"mx Matrix interest n Number principal components / varimax factors return; can take vector values prcomp_fn Function use extract principal components prcomp_opts List options pass prcomp_fn varimax_fn Function use varimax rotation varimax_opts List options pass varimax_fn","code":""},{"path":"https://dhicks.github.io/tmfast/reference/varimax_irlba.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a varimax-rotated PCA using irlba — varimax_irlba","text":"list class varimaxes, elements totalvar: Total variance, PCA sdev:  Standard deviations extracted principal components rotation:  Rotation matrix (variable loadings) PCA varimaxes: list class varimaxes, containing one fitted varimax model value n, elements loadings: Varimax-rotated standardized loadings rotmat:  Varimax rotation matrix scores:  Varimax-rotated observation scores","code":""}]
